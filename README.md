
# ğŸŒ³ AI-Powered Urban Green Space Management System (UGSMS)

An end-to-end machine learning pipeline built on Databricks to optimize park-level interventions in German cities using environmental, usage, and sentiment data.

---

## ğŸš€ Project Overview

The **Urban Green Space Management System (UGSMS)** is designed to help city authorities and park managers make **data-driven decisions** by integrating multi-source data (air quality, footfall, sentiment, and geographic info) and applying ML to recommend timely interventions.

---

## ğŸ”‘ Key Features

- âœ… **Multi-source Data Integration**: Ingests air quality, footfall, sentiment, and park data.
- âš¡ **Real-time Data Processing**: Uses Apache Spark for scalable and efficient processing.
- ğŸ§  **ML Pipeline**: Logistic Regression, Random Forest, and Gradient Boosting for prediction.
- ğŸ›ï¸ **Delta Lake Architecture**: Bronze â†’ Silver â†’ Gold medallion model.
- ğŸ“ˆ **MLflow Integration**: Tracks experiments, models, and metrics.
- ğŸ¯ **Automated Recommendations**: Actionable outputs for park improvement planning.

---

## ğŸŒ Project Value

- ğŸŒ± **Proactive Park Management**
- ğŸ”§ **Efficient Resource Allocation**
- ğŸ“Š **Data-Driven Decision Making**
- ğŸƒ **Enhanced Environmental Impact**

---

## ğŸ“¦ Architecture

![UGSMS Architecture](./docs/Architecture.png)

### Step 1: Data Ingestion andÂ Cleaning
The first step in developing the UGSMS is to ingest and clean the data from various sources.

**Note on Data:** For the purpose of this project, synthetic data was generated to simulate real-world environmental, usage, and sentiment patterns. These data files are stored in a GitHub repository and are used directly for ingestion into the system.

* **Air Quality Data** (`national_parks_air_quality.csv`)
    * Air Quality Index (AQI)
    * NO2, PM2.5, O3 levels
* **Footfall Data** (`national_parks_footfall.csv`)
    * Visitor counts
    * Event day indicators
* **Sentiment Data** (`national_parks_sentiment.csv`)
    * Tweet text analysis
    * Sentiment scores (-1 to 1)
    * Sentiment labels
* **Parks Information** (`german_national_parks.csv`)
    * Geographic coordinates
    * Area measurements

The data ingestion process involves reading the CSV files into Spark DataFrames using the `read_data` function.

Functions:
```python
def read_data(url): ...
def clean_data(df): ...
```

---

### 2. Data Storage: Medallion Architecture
- **Bronze**: Raw source data
- **Silver**: Cleaned and joined datasets
- **Gold**: Aggregated features and ML predictions

---

### 3. Feature Engineering
Aggregates metrics such as AQI, sentiment, visitor counts, and events per park using:
```python
def create_aggregated_features(df_spark): ...
```

---

### 4. Target Variable
Defines `intervention_required` based on business rules:
```python
def create_target_variable(df_spark): ...
```

---

### 5. Machine Learning Pipeline

ML Algorithms Used:
- Logistic Regression
- Random Forest
- Gradient Boosting

Each model is built using:
```python
def create_ml_pipelines(): ...
```

Hyperparameter tuning via `GridSearchCV` with custom `param_grids`.

---

### 6. Model Evaluation & Comparison

Evaluation metrics:
- Accuracy
- Precision
- Recall
- F1-Score
- AUC-ROC

Visual tools include:
- Confusion Matrix
- ROC & PR Curves
- Comparison Bar & Scatter Plots

```python
def evaluate_model_performance(...): ...
def create_model_comparison_visualization(...): ...
```

---

### 7. MLflow Integration
All models and experiments are logged using:
```python
with mlflow.start_run(...): ...
```

---

### 8. Feature Importance
Visualizes which features influence predictions most:
```python
def analyze_feature_importance(...): ...
```

---

### 9. Saving & Visualizing Predictions

Model outputs are saved to Spark tables:
```python
def save_predictions_to_spark(...): ...
```

---

### ğŸ”® Recommendation Engine
Recommends actions based on prediction probabilities:
```python
def generate_recommendation(prob, city): ...
```

---

## ğŸ“Š Demo & Visualizations

Coming soon:  
âœ… Model performance charts  
âœ… Feature importance plots  
âœ… Intervention recommendation maps

---

## ğŸ“ Project Structure

```bash
ğŸ“¦ ugsms/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 1_data_ingestion.py
â”‚   â”œâ”€â”€ 2_data_cleaning.py
â”‚   â”œâ”€â”€ 3_feature_engineering.py
â”‚   â”œâ”€â”€ 4_ml_pipeline.py
â”‚   â”œâ”€â”€ 5_model_evaluation.py
â”‚   â””â”€â”€ 6_recommendations.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ *.csv (synthetic data)
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ› ï¸ Installation

This project is built for **Databricks Community Edition**.

1. Clone the repo:
```bash
git clone https://github.com/yourusername/UGSMS.git
```
2. Upload notebooks to Databricks
3. Create a cluster with:
   - Runtime: 12.x LTS (Scala 2.12, Spark 3.x)
4. Run notebooks sequentially

---

## ğŸ¤ Contributing

Pull requests are welcome! Please fork the repo and open a PR.

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

## ğŸ“¬ Contact

Made with â¤ï¸ by **Dilshan Chanuka**  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/dilshan-chanuka/)  
