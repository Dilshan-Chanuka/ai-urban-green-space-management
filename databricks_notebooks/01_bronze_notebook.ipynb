{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a61e8353-c0f5-4f61-906d-6b8fae24f15f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. INGEST & CLEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9170983d-f6f0-44b5-8766-d8ac29cd566e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753711344925}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# URLs for the datasets\n",
    "# aq_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/air_quality.csv\"\n",
    "# foot_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/footfall.csv\"\n",
    "# sent_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/sentiment.csv\"\n",
    "# parks_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/parks.csv\"\n",
    "\n",
    "aq_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/national_parks_air_quality.csv\"\n",
    "foot_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/national_parks_footfall.csv\"\n",
    "sent_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/national_parks_sentiment.csv\"\n",
    "parks_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/german_national_parks.csv\"\n",
    "\n",
    "# Read CSVs using pandas\n",
    "aq_df_pd = pd.read_csv(aq_url)\n",
    "foot_df_pd = pd.read_csv(foot_url)\n",
    "sent_df_pd = pd.read_csv(sent_url)\n",
    "parks_df_pd = pd.read_csv(parks_url)\n",
    "\n",
    "# Convert to Spark DataFrames\n",
    "aq_df = spark.createDataFrame(aq_df_pd)\n",
    "foot_df = spark.createDataFrame(foot_df_pd)\n",
    "sent_df = spark.createDataFrame(sent_df_pd)\n",
    "parks_df = spark.createDataFrame(parks_df_pd)\n",
    "\n",
    "\n",
    "# # Optionally display the tables\n",
    "# display(spark.read.table(\"ml_project.bronze.air_quality\"))\n",
    "# display(spark.read.table(\"ml_project.bronze.footfall\"))\n",
    "# display(spark.read.table(\"ml_project.bronze.sentiment\"))\n",
    "# display(spark.read.table(\"ml_project.bronze.parks\"))\n",
    "\n",
    "\n",
    "# # Load datasets\n",
    "# aq_df = spark.read.table(\"ml_project.bronze.air_quality\")\n",
    "# foot_df = spark.read.table(\"ml_project.bronze.footfall\")\n",
    "# sent_df = spark.read.table(\"ml_project.bronze.sentiment\")\n",
    "# parks_df = spark.read.table(\"ml_project.bronze.parks\")\n",
    "\n",
    "# Data cleaning\n",
    "aq_df = aq_df.dropna().withColumn(\"AQI\", col(\"AQI\").cast(\"float\"))\n",
    "foot_df = foot_df.dropna().withColumn(\"visitor_count\", col(\"visitor_count\").cast(\"integer\"))\n",
    "sent_df = sent_df.dropna()\n",
    "parks_df = parks_df.dropna()\n",
    "\n",
    "# Write as Delta Tables\n",
    "aq_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"ml_project.bronze.air_quality\")\n",
    "\n",
    "foot_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"ml_project.bronze.footfall\")\n",
    "\n",
    "sent_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"ml_project.bronze.sentiment\")\n",
    "\n",
    "parks_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"ml_project.bronze.parks\")\n",
    "\n",
    "\n",
    "display(aq_df)\n",
    "display(foot_df)\n",
    "display(sent_df)\n",
    "display(parks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c1bf643-325c-4062-989e-850e422a0997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# URLs for the datasets\n",
    "aq_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/national_parks_air_quality.csv\"\n",
    "foot_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/national_parks_footfall.csv\"\n",
    "sent_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/national_parks_sentiment.csv\"\n",
    "parks_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/german_national_parks.csv\"\n",
    "\n",
    "# Read CSVs using pandas\n",
    "aq_df_pd = pd.read_csv(aq_url)\n",
    "foot_df_pd = pd.read_csv(foot_url)\n",
    "sent_df_pd = pd.read_csv(sent_url)\n",
    "parks_df_pd = pd.read_csv(parks_url)\n",
    "\n",
    "# Convert to Spark DataFrames\n",
    "aq_df = spark.createDataFrame(aq_df_pd)\n",
    "foot_df = spark.createDataFrame(foot_df_pd)\n",
    "sent_df = spark.createDataFrame(sent_df_pd)\n",
    "parks_df = spark.createDataFrame(parks_df_pd)\n",
    "\n",
    "# Data cleaning and formatting\n",
    "aq_df = aq_df.dropna()\n",
    "foot_df = foot_df.dropna() \\\n",
    "                 .withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), \"M/d/yyyy H:mm\")) \\\n",
    "                 .withColumn(\"visitor_count\", col(\"visitor_count\").cast(\"integer\")) \\\n",
    "                 .withColumn(\"event_day\", col(\"event_day\").cast(\"boolean\"))\n",
    "\n",
    "sent_df = sent_df.dropna()\n",
    "parks_df = parks_df.dropna()\n",
    "\n",
    "# Optionally display the DataFrames\n",
    "print(\"Air Quality DataFrame:\")\n",
    "aq_df.show()\n",
    "\n",
    "print(\"Footfall DataFrame:\")\n",
    "foot_df.show()\n",
    "\n",
    "print(\"Sentiment DataFrame:\")\n",
    "sent_df.show()\n",
    "\n",
    "print(\"Parks DataFrame:\")\n",
    "parks_df.show()\n",
    "\n",
    "# Write as Delta Tables\n",
    "aq_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"ml_project.bronze.air_quality\")\n",
    "\n",
    "foot_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"ml_project.bronze.footfall\")\n",
    "\n",
    "sent_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"ml_project.bronze.sentiment\")\n",
    "\n",
    "parks_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"ml_project.bronze.parks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7e50527-fc62-4698-a689-55fdb0cc3637",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1753782679523}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, sum\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# URLs for the datasets\n",
    "aq_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/national_parks_air_quality.csv\"\n",
    "foot_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/national_parks_footfall.csv\"\n",
    "sent_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/national_parks_sentiment.csv\"\n",
    "parks_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/german_national_parks.csv\"\n",
    "\n",
    "# Read CSVs using pandas\n",
    "aq_df_pd = pd.read_csv(aq_url)\n",
    "foot_df_pd = pd.read_csv(foot_url)\n",
    "sent_df_pd = pd.read_csv(sent_url)\n",
    "parks_df_pd = pd.read_csv(parks_url)\n",
    "\n",
    "# Convert to Spark DataFrames\n",
    "aq_df = spark.createDataFrame(aq_df_pd)\n",
    "foot_df = spark.createDataFrame(foot_df_pd)\n",
    "sent_df = spark.createDataFrame(sent_df_pd)\n",
    "parks_df = spark.createDataFrame(parks_df_pd)\n",
    "\n",
    "# Data cleaning and formatting\n",
    "aq_df = aq_df.dropna()\n",
    "\n",
    "# Rename the timestamp column in footfall data to avoid conflicts\n",
    "foot_df = foot_df.dropna() \\\n",
    "                 .withColumnRenamed(\"timestamp\", \"footfall_timestamp\") \\\n",
    "                 .withColumn(\"footfall_timestamp\", to_timestamp(col(\"footfall_timestamp\"), \"M/d/yyyy H:mm\")) \\\n",
    "                 .withColumn(\"visitor_count\", col(\"visitor_count\").cast(\"integer\")) \\\n",
    "                 .withColumn(\"event_day\", col(\"event_day\").cast(\"boolean\"))\n",
    "\n",
    "sent_df = sent_df.dropna()\n",
    "parks_df = parks_df.dropna()\n",
    "\n",
    "# Calculate total visitors for each park\n",
    "visitor_count_df = foot_df.groupBy(\"park_id\").sum(\"visitor_count\").withColumnRenamed(\"sum(visitor_count)\", \"total_visitors\")\n",
    "\n",
    "# Join DataFrames based on 'park_id'\n",
    "combined_df = parks_df.join(aq_df, on='park_id', how='left') \\\n",
    "                     .join(foot_df, on='park_id', how='left') \\\n",
    "                     .join(sent_df, on='park_id', how='left') \\\n",
    "                     .join(visitor_count_df, on='park_id', how='left')\n",
    "\n",
    "# Optionally display the combined DataFrame\n",
    "print(\"Combined DataFrame:\")\n",
    "display(combined_df)\n",
    "\n",
    "# Write as Delta Table\n",
    "combined_df.write.format(\"delta\") \\\n",
    "           .mode(\"overwrite\") \\\n",
    "           .option(\"overwriteSchema\", \"true\") \\\n",
    "           .saveAsTable(\"ml_project.bronze.park_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8dae1e7-708a-4309-94cb-8cfd197b7774",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753782771335}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"ml_project.bronze.footfall\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ed94786-b070-46aa-82e4-b76aab8b94fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8963494350797170,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
