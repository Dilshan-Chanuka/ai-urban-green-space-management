{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8dae1e7-708a-4309-94cb-8cfd197b7774",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753782771335}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Create a SparkSession with explicit configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Urban Green Space Management System\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# URLs for the datasets\n",
    "aq_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/national_parks_air_quality.csv\"\n",
    "foot_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/national_parks_footfall.csv\"\n",
    "sent_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/national_parks_sentiment.csv\"\n",
    "parks_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/german_national_parks.csv\"\n",
    "\n",
    "def read_data(url):\n",
    "    \n",
    "    return spark.createDataFrame(pd.read_csv(url))\n",
    "    \n",
    "def clean_data(df):\n",
    "    \n",
    "    df = df.dropna(how='any', subset=[col for col in df.columns if col != ''])\n",
    "    if 'timestamp' in df.columns:\n",
    "        df = df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), \"M/d/yyyy H:mm\"))\n",
    "    if 'visitor_count' in df.columns:\n",
    "        df = df.withColumn(\"visitor_count\", col(\"visitor_count\").cast(\"integer\"))\n",
    "    if 'event_day' in df.columns:\n",
    "        df = df.withColumn(\"event_day\", col(\"event_day\").cast(\"boolean\"))\n",
    "    return df\n",
    "\n",
    "def write_to_delta(df, table_name):\n",
    "    \n",
    "    try:\n",
    "        df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(table_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to Delta table {table_name}: {e}\")\n",
    "\n",
    "# Read and clean data\n",
    "aq_df = clean_data(read_data(aq_url))\n",
    "foot_df = clean_data(read_data(foot_url))\n",
    "sent_df = clean_data(read_data(sent_url))\n",
    "parks_df = clean_data(read_data(parks_url))\n",
    "\n",
    "# Write to Delta tables\n",
    "write_to_delta(aq_df, \"ml_project.bronze.air_quality\")\n",
    "write_to_delta(foot_df, \"ml_project.bronze.footfall\")\n",
    "write_to_delta(sent_df, \"ml_project.bronze.sentiment\")\n",
    "write_to_delta(parks_df, \"ml_project.bronze.parks\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8963494350797170,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
