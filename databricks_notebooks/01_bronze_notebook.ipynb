{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a61e8353-c0f5-4f61-906d-6b8fae24f15f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. INGEST & CLEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9170983d-f6f0-44b5-8766-d8ac29cd566e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753711344925}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# URLs for the datasets\n",
    "aq_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/air_quality.csv\"\n",
    "foot_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/footfall.csv\"\n",
    "sent_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/sentiment.csv\"\n",
    "parks_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/parks.csv\"\n",
    "\n",
    "# Read CSVs using pandas\n",
    "aq_df_pd = pd.read_csv(aq_url)\n",
    "foot_df_pd = pd.read_csv(foot_url)\n",
    "sent_df_pd = pd.read_csv(sent_url)\n",
    "parks_df_pd = pd.read_csv(parks_url)\n",
    "\n",
    "# Convert to Spark DataFrames\n",
    "aq_df = spark.createDataFrame(aq_df_pd)\n",
    "foot_df = spark.createDataFrame(foot_df_pd)\n",
    "sent_df = spark.createDataFrame(sent_df_pd)\n",
    "parks_df = spark.createDataFrame(parks_df_pd)\n",
    "\n",
    "# Write as Delta Tables\n",
    "aq_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"ml_project.bronze.air_quality\")\n",
    "\n",
    "foot_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"ml_project.bronze.footfall\")\n",
    "\n",
    "sent_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"ml_project.bronze.sentiment\")\n",
    "\n",
    "parks_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"ml_project.bronze.parks\")\n",
    "\n",
    "# # Optionally display the tables\n",
    "# display(spark.read.table(\"ml_project.bronze.air_quality\"))\n",
    "# display(spark.read.table(\"ml_project.bronze.footfall\"))\n",
    "# display(spark.read.table(\"ml_project.bronze.sentiment\"))\n",
    "# display(spark.read.table(\"ml_project.bronze.parks\"))\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "aq_df = spark.read.table(\"ml_project.default.air_quality\")\n",
    "foot_df = spark.read.table(\"ml_project.default.footfall\")\n",
    "sent_df = spark.read.table(\"ml_project.default.sentiment\")\n",
    "parks_df = spark.read.table(\"ml_project.default.parks\")\n",
    "\n",
    "# Data cleaning\n",
    "aq_df = aq_df.dropna().withColumn(\"AQI\", col(\"AQI\").cast(\"float\"))\n",
    "foot_df = foot_df.dropna().withColumn(\"visitor_count\", col(\"visitor_count\").cast(\"integer\"))\n",
    "sent_df = sent_df.dropna()\n",
    "parks_df = parks_df.dropna()\n",
    "\n",
    "\n",
    "display(aq_df)\n",
    "display(foot_df)\n",
    "display(sent_df)\n",
    "display(parks_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8963494350797170,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
