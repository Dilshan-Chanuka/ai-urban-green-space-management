{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a61e8353-c0f5-4f61-906d-6b8fae24f15f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. INGEST & CLEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8dae1e7-708a-4309-94cb-8cfd197b7774",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753782771335}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Create a SparkSession with explicit configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Urban Green Space Management System\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# URLs for the datasets\n",
    "aq_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/national_parks_air_quality.csv\"\n",
    "foot_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/national_parks_footfall.csv\"\n",
    "sent_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/national_parks_sentiment.csv\"\n",
    "parks_url = \"https://raw.githubusercontent.com/Dilshan-Chanuka/ml-anomaly-detection-pipeline/refs/heads/main/data_sample/german_national_parks.csv\"\n",
    "\n",
    "def read_data(url):\n",
    "    \"\"\"Read CSV data from a URL into a pandas DataFrame and then convert it to a Spark DataFrame\"\"\"\n",
    "    return spark.createDataFrame(pd.read_csv(url))\n",
    "    \n",
    "def clean_data(df):\n",
    "    \"\"\"Clean and format the DataFrame\"\"\"\n",
    "    df = df.dropna(how='any', subset=[col for col in df.columns if col != ''])\n",
    "    if 'timestamp' in df.columns:\n",
    "        df = df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), \"M/d/yyyy H:mm\"))\n",
    "    if 'visitor_count' in df.columns:\n",
    "        df = df.withColumn(\"visitor_count\", col(\"visitor_count\").cast(\"integer\"))\n",
    "    if 'event_day' in df.columns:\n",
    "        df = df.withColumn(\"event_day\", col(\"event_day\").cast(\"boolean\"))\n",
    "    return df\n",
    "\n",
    "def write_to_delta(df, table_name):\n",
    "    \"\"\"Write the DataFrame to a Delta table\"\"\"\n",
    "    try:\n",
    "        df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(table_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to Delta table {table_name}: {e}\")\n",
    "\n",
    "# Read and clean data\n",
    "aq_df = clean_data(read_data(aq_url))\n",
    "foot_df = clean_data(read_data(foot_url))\n",
    "sent_df = clean_data(read_data(sent_url))\n",
    "parks_df = clean_data(read_data(parks_url))\n",
    "\n",
    "# Optionally display the DataFrames\n",
    "display(aq_df)\n",
    "display(foot_df)\n",
    "display(sent_df)\n",
    "display(parks_df)\n",
    "\n",
    "# Write to Delta tables\n",
    "write_to_delta(aq_df, \"ml_project.bronze.air_quality\")\n",
    "write_to_delta(foot_df, \"ml_project.bronze.footfall\")\n",
    "write_to_delta(sent_df, \"ml_project.bronze.sentiment\")\n",
    "write_to_delta(parks_df, \"ml_project.bronze.parks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2ec1166-5fdd-43ea-b4ec-9fbd386114f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform EDA on the DataFrames\n",
    "print(\"Air Quality DataFrame Summary:\")\n",
    "aq_df.describe().show()\n",
    "\n",
    "print(\"Footfall DataFrame Summary:\")\n",
    "foot_df.describe().show()\n",
    "\n",
    "print(\"Sentiment DataFrame Summary:\")\n",
    "sent_df.describe().show()\n",
    "\n",
    "print(\"Parks DataFrame Summary:\")\n",
    "parks_df.describe().show()\n",
    "\n",
    "# Visualize the data using plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the distribution of visitor_count\n",
    "foot_df_pd = foot_df.toPandas()\n",
    "plt.hist(foot_df_pd['visitor_count'], bins=50)\n",
    "plt.title('Distribution of Visitor Count')\n",
    "plt.xlabel('Visitor Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Plot the distribution of sentiment_score\n",
    "sent_df_pd = sent_df.toPandas()\n",
    "plt.hist(sent_df_pd['sentiment_score'], bins=50)\n",
    "plt.title('Distribution of Sentiment Score')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2c516dd-450a-4128-878e-dce833cbcd35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create new features\n",
    "from pyspark.sql.functions import dayofweek, hour\n",
    "\n",
    "# Extract day of week and hour from timestamp\n",
    "foot_df = foot_df.withColumn(\"day_of_week\", dayofweek(col(\"timestamp\"))) \\\n",
    "                  .withColumn(\"hour\", hour(col(\"timestamp\")))\n",
    "\n",
    "# Create a new feature: average visitor count per day of week\n",
    "avg_visitor_count_per_day = foot_df.groupBy(\"day_of_week\").agg({\"visitor_count\": \"mean\"}).withColumnRenamed(\"avg(visitor_count)\", \"avg_visitor_count\")\n",
    "foot_df = foot_df.join(avg_visitor_count_per_day, on=\"day_of_week\", how=\"left\")\n",
    "\n",
    "# Create a new feature: sentiment score per park\n",
    "sentiment_score_per_park = sent_df.groupBy(\"park_id\").agg({\"sentiment_score\": \"mean\"}).withColumnRenamed(\"avg(sentiment_score)\", \"avg_sentiment_score\")\n",
    "parks_df = parks_df.join(sentiment_score_per_park, on=\"park_id\", how=\"left\")\n",
    "\n",
    "display(foot_df)\n",
    "display(parks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e92b73-57da-4ecc-bbc5-ed06d71ea0a5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753873055930}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, count, when, col, dayofweek, hour, mean\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Convert the date column to date type\n",
    "aq_df = aq_df.withColumn(\"date\", to_date(col(\"date\"), \"M/d/yyyy\"))\n",
    "\n",
    "# Convert the timestamp column to date type\n",
    "foot_df = foot_df.withColumn(\"date\", to_date(col(\"timestamp\")))\n",
    "\n",
    "# Create new features\n",
    "foot_df = foot_df.withColumn(\"day_of_week\", dayofweek(col(\"timestamp\"))) \\\n",
    "                  .withColumn(\"hour\", hour(col(\"timestamp\")))\n",
    "\n",
    "# Create a new feature: average visitor count per day of week\n",
    "avg_visitor_count_per_day = foot_df.groupBy(\"day_of_week\").agg(F.mean(\"visitor_count\").alias(\"avg_visitor_count\"))\n",
    "foot_df = foot_df.join(avg_visitor_count_per_day, on=\"day_of_week\", how=\"left\")\n",
    "\n",
    "# Create a new feature: sentiment score per park\n",
    "sentiment_score_per_park = sent_df.groupBy(\"park_id\").agg(F.mean(\"sentiment_score\").alias(\"avg_sentiment_score\"))\n",
    "parks_df = parks_df.join(sentiment_score_per_park, on=\"park_id\", how=\"left\")\n",
    "\n",
    "# Integrate the DataFrames\n",
    "integrated_df = foot_df.join(parks_df, on=\"park_id\", how=\"left\") \\\n",
    "                        .join(aq_df, on=[\"park_id\", \"date\"], how=\"left\") \\\n",
    "                        .join(sent_df, on=[\"park_id\", \"timestamp\"], how=\"left\")\n",
    "\n",
    "# Rename columns to avoid invalid characters\n",
    "for col_name in integrated_df.columns:\n",
    "    if '(' in col_name or ')' in col_name:\n",
    "        new_col_name = col_name.replace('(', '_').replace(')', '_')\n",
    "        integrated_df = integrated_df.withColumnRenamed(col_name, new_col_name)\n",
    "\n",
    "# Drop duplicate columns\n",
    "for col_name in integrated_df.columns:\n",
    "    if integrated_df.columns.count(col_name) > 1:\n",
    "        integrated_df = integrated_df.drop(col_name)\n",
    "\n",
    "# Save the integrated DataFrame as a Delta table\n",
    "write_to_delta(integrated_df, \"ml_project.silver.integrated\")\n",
    "display(integrated_df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8963494350797170,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
