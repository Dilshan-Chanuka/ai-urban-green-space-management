{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f33d93a-ecca-4bd5-a0f4-575980110565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced Urban Green Space Management System\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    avg, sum, min, max, stddev, count, col, when, isnan, isnull, \n",
    "    date_format, hour, dayofweek, month, percentile_approx,\n",
    "    regexp_extract, split, size, desc, asc, lit\n",
    ")\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== URBAN GREEN SPACE MANAGEMENT SYSTEM ===\")\n",
    "print(\"Feature Engineering & Exploratory Data Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. ENHANCED FEATURE ENGINEERING & ENRICHMENT\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_validate_data():\n",
    "    \"\"\"Load datasets with validation and error handling\"\"\"\n",
    "    try:\n",
    "        print(\"\\n Loading datasets...\")\n",
    "        \n",
    "        # Load datasets\n",
    "        aq_df = spark.read.table(\"ml_project.bronze.air_quality\")\n",
    "        foot_df = spark.read.table(\"ml_project.bronze.footfall\")\n",
    "        sent_df = spark.read.table(\"ml_project.bronze.sentiment\")\n",
    "        parks_df = spark.read.table(\"ml_project.bronze.parks\")\n",
    "        \n",
    "        # Validate data\n",
    "        datasets = {\n",
    "            'Air Quality': aq_df,\n",
    "            'Footfall': foot_df,\n",
    "            'Sentiment': sent_df,\n",
    "            'Parks': parks_df\n",
    "        }\n",
    "        \n",
    "        for name, df in datasets.items():\n",
    "            count = df.count()\n",
    "            print(f\"   {name}: {count:,} records\")\n",
    "            \n",
    "            # Check for nulls\n",
    "            null_counts = []\n",
    "            for column in df.columns:\n",
    "                null_count = df.filter(col(column).isNull()).count()\n",
    "                if null_count > 0:\n",
    "                    null_counts.append(f\"{column}: {null_count}\")\n",
    "            \n",
    "            if null_counts:\n",
    "                print(f\"      Null values: {', '.join(null_counts)}\")\n",
    "        \n",
    "        return aq_df, foot_df, sent_df, parks_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def create_comprehensive_features(aq_df, foot_df, sent_df, parks_df):\n",
    "    \"\"\"Create comprehensive feature set with temporal and statistical features\"\"\"\n",
    "    \n",
    "    print(\"\\n Creating comprehensive features...\")\n",
    "    \n",
    "    try:\n",
    "        # Enhanced Air Quality Metrics\n",
    "        print(\"   Processing air quality metrics...\")\n",
    "        aqi_metrics = aq_df.groupBy(\"park_id\").agg(\n",
    "            avg(\"AQI\").alias(\"avg_aqi\"),\n",
    "            min(\"AQI\").alias(\"min_aqi\"),\n",
    "            max(\"AQI\").alias(\"max_aqi\"),\n",
    "            stddev(\"AQI\").alias(\"aqi_volatility\"),\n",
    "            percentile_approx(\"AQI\", 0.5).alias(\"median_aqi\"),\n",
    "            avg(\"no2_level\").alias(\"avg_no2\"),\n",
    "            avg(\"pm25_level\").alias(\"avg_pm25\"),\n",
    "            avg(\"o3_level\").alias(\"avg_o3\"),\n",
    "            count(\"*\").alias(\"aqi_measurements\")\n",
    "        )\n",
    "        \n",
    "        # Enhanced Footfall Metrics with Temporal Analysis\n",
    "        print(\"   Processing footfall metrics...\")\n",
    "        \n",
    "        # Check if timestamp column exists and try temporal features\n",
    "        footfall_columns = foot_df.columns\n",
    "        \n",
    "        if \"timestamp\" in footfall_columns:\n",
    "            try:\n",
    "                # Add temporal features to footfall data\n",
    "                foot_temporal = foot_df.withColumn(\"hour\", hour(col(\"timestamp\"))) \\\n",
    "                                      .withColumn(\"day_of_week\", dayofweek(col(\"timestamp\"))) \\\n",
    "                                      .withColumn(\"month\", month(col(\"timestamp\"))) \\\n",
    "                                      .withColumn(\"is_weekend\", when(col(\"day_of_week\").isin([1, 7]), 1).otherwise(0)) \\\n",
    "                                      .withColumn(\"is_event_day\", when(col(\"event_day\") == True, 1).otherwise(0))\n",
    "            except Exception as temporal_error:\n",
    "                print(f\"      Temporal features failed: {str(temporal_error)}\")\n",
    "                foot_temporal = foot_df.withColumn(\"is_weekend\", lit(0)) \\\n",
    "                                      .withColumn(\"is_event_day\", when(col(\"event_day\") == True, 1).otherwise(0))\n",
    "        else:\n",
    "            print(\"      No timestamp column found, using basic footfall metrics\")\n",
    "            foot_temporal = foot_df.withColumn(\"is_weekend\", lit(0)) \\\n",
    "                                  .withColumn(\"is_event_day\", when(col(\"event_day\") == True, 1).otherwise(0))\n",
    "        \n",
    "        foot_metrics = foot_temporal.groupBy(\"park_id\").agg(\n",
    "            sum(\"visitor_count\").alias(\"total_footfall\"),\n",
    "            avg(\"visitor_count\").alias(\"avg_footfall\"),\n",
    "            max(\"visitor_count\").alias(\"peak_footfall\"),\n",
    "            stddev(\"visitor_count\").alias(\"footfall_volatility\"),\n",
    "            sum(\"is_weekend\").alias(\"weekend_visits\"),\n",
    "            sum(\"is_event_day\").alias(\"event_days\"),\n",
    "            count(\"*\").alias(\"footfall_records\")\n",
    "        )\n",
    "        \n",
    "        # Peak hour analysis (if temporal features worked)\n",
    "        try:\n",
    "            if \"hour\" in foot_temporal.columns:\n",
    "                peak_hours = foot_temporal.groupBy(\"park_id\", \"hour\") \\\n",
    "                                         .agg(avg(\"visitor_count\").alias(\"avg_hourly_visitors\")) \\\n",
    "                                         .groupBy(\"park_id\") \\\n",
    "                                         .agg(max(\"avg_hourly_visitors\").alias(\"peak_hour_avg\"))\n",
    "                foot_metrics = foot_metrics.join(peak_hours, \"park_id\", \"left\")\n",
    "            else:\n",
    "                foot_metrics = foot_metrics.withColumn(\"peak_hour_avg\", lit(0.0))\n",
    "        except Exception as peak_error:\n",
    "            print(f\"      Peak hour analysis failed: {str(peak_error)}\")\n",
    "            foot_metrics = foot_metrics.withColumn(\"peak_hour_avg\", lit(0.0))\n",
    "        \n",
    "        # Enhanced Sentiment Metrics\n",
    "        print(\"   Processing sentiment metrics...\")\n",
    "        sent_metrics = sent_df.groupBy(\"park_id\").agg(\n",
    "            avg(\"sentiment_score\").alias(\"avg_sentiment\"),\n",
    "            min(\"sentiment_score\").alias(\"min_sentiment\"),\n",
    "            max(\"sentiment_score\").alias(\"max_sentiment\"),\n",
    "            stddev(\"sentiment_score\").alias(\"sentiment_volatility\"),\n",
    "            sum(when(col(\"sentiment_label\") == \"positive\", 1).otherwise(0)).alias(\"positive_count\"),\n",
    "            sum(when(col(\"sentiment_label\") == \"negative\", 1).otherwise(0)).alias(\"negative_count\"),\n",
    "            sum(when(col(\"sentiment_label\") == \"neutral\", 1).otherwise(0)).alias(\"neutral_count\"),\n",
    "            count(\"*\").alias(\"total_mentions\")\n",
    "        )\n",
    "        \n",
    "        # Calculate sentiment ratios\n",
    "        sent_metrics = sent_metrics.withColumn(\n",
    "            \"positive_ratio\", col(\"positive_count\") / col(\"total_mentions\")\n",
    "        ).withColumn(\n",
    "            \"negative_ratio\", col(\"negative_count\") / col(\"total_mentions\")\n",
    "        ).withColumn(\n",
    "            \"sentiment_polarity\", (col(\"positive_count\") - col(\"negative_count\")) / col(\"total_mentions\")\n",
    "        )\n",
    "        \n",
    "        # Merge all features\n",
    "        print(\"   Merging all features...\")\n",
    "        features_df = parks_df.join(aqi_metrics, \"park_id\", \"left\") \\\n",
    "                             .join(foot_metrics, \"park_id\", \"left\") \\\n",
    "                             .join(sent_metrics, \"park_id\", \"left\")\n",
    "        \n",
    "        # Create derived features\n",
    "        print(\"  • Creating derived features...\")\n",
    "        features_df = features_df.withColumn(\n",
    "            \"pollution_footfall_ratio\", \n",
    "            when(col(\"total_footfall\") > 0, col(\"avg_aqi\") / col(\"total_footfall\")).otherwise(0)\n",
    "        ).withColumn(\n",
    "            \"area_efficiency\", \n",
    "            when(col(\"area_sqm\") > 0, col(\"total_footfall\") / col(\"area_sqm\") * 1000).otherwise(0)  # visitors per 1000 sqm\n",
    "        ).withColumn(\n",
    "            \"sentiment_engagement_ratio\",\n",
    "            when(col(\"total_footfall\") > 0, col(\"total_mentions\") / col(\"total_footfall\") * 100).otherwise(0)\n",
    "        ).withColumn(\n",
    "            \"air_quality_category\",\n",
    "            when(col(\"avg_aqi\") <= 50, \"Good\")\n",
    "            .when(col(\"avg_aqi\") <= 100, \"Moderate\")\n",
    "            .when(col(\"avg_aqi\") <= 150, \"Unhealthy for Sensitive\")\n",
    "            .when(col(\"avg_aqi\") <= 200, \"Unhealthy\")\n",
    "            .otherwise(\"Very Unhealthy\")\n",
    "        ).withColumn(\n",
    "            \"usage_category\",\n",
    "            when(col(\"total_footfall\") < 100, \"Low\")\n",
    "            .when(col(\"total_footfall\") < 500, \"Medium\")\n",
    "            .when(col(\"total_footfall\") < 1000, \"High\")\n",
    "            .otherwise(\"Very High\")\n",
    "        ).withColumn(\n",
    "            \"park_size_category\",\n",
    "            when(col(\"area_sqm\") < 50000, \"Small\")\n",
    "            .when(col(\"area_sqm\") < 200000, \"Medium\")\n",
    "            .when(col(\"area_sqm\") < 500000, \"Large\")\n",
    "            .otherwise(\"Very Large\")\n",
    "        )\n",
    "        \n",
    "        # Fill null values with appropriate bronzes\n",
    "        fill_values = {\n",
    "            \"avg_aqi\": 0.0,\n",
    "            \"total_footfall\": 0,\n",
    "            \"avg_sentiment\": 0.0,\n",
    "            \"aqi_measurements\": 0,\n",
    "            \"footfall_records\": 0,\n",
    "            \"total_mentions\": 0,\n",
    "            \"min_aqi\": 0.0,\n",
    "            \"max_aqi\": 0.0,\n",
    "            \"aqi_volatility\": 0.0,\n",
    "            \"median_aqi\": 0.0,\n",
    "            \"avg_footfall\": 0.0,\n",
    "            \"peak_footfall\": 0,\n",
    "            \"footfall_volatility\": 0.0,\n",
    "            \"weekend_visits\": 0,\n",
    "            \"event_days\": 0,\n",
    "            \"peak_hour_avg\": 0.0,\n",
    "            \"positive_count\": 0,\n",
    "            \"negative_count\": 0,\n",
    "            \"neutral_count\": 0,\n",
    "            \"positive_ratio\": 0.0,\n",
    "            \"negative_ratio\": 0.0,\n",
    "            \"sentiment_polarity\": 0.0,\n",
    "            \"pollution_footfall_ratio\": 0.0,\n",
    "            \"area_efficiency\": 0.0,\n",
    "            \"sentiment_engagement_ratio\": 0.0\n",
    "        }\n",
    "        \n",
    "        features_df = features_df.fillna(fill_values)\n",
    "        \n",
    "        print(f\"   Created feature set with {len(features_df.columns)} columns\")\n",
    "        \n",
    "        return features_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error in feature engineering: {str(e)}\")\n",
    "        print(\" Attempting simplified feature engineering...\")\n",
    "        \n",
    "        # Fallback to basic feature engineering\n",
    "        try:\n",
    "            # Basic aggregations only\n",
    "            aqi_basic = aq_df.groupBy(\"park_id\").agg(avg(\"AQI\").alias(\"avg_aqi\"))\n",
    "            foot_basic = foot_df.groupBy(\"park_id\").agg(sum(\"visitor_count\").alias(\"total_footfall\"))\n",
    "            sent_basic = sent_df.groupBy(\"park_id\").agg(avg(\"sentiment_score\").alias(\"avg_sentiment\"))\n",
    "            \n",
    "            # Basic merge\n",
    "            features_basic = parks_df.join(aqi_basic, \"park_id\", \"left\") \\\n",
    "                                   .join(foot_basic, \"park_id\", \"left\") \\\n",
    "                                   .join(sent_basic, \"park_id\", \"left\")\n",
    "            \n",
    "            # Basic categories\n",
    "            features_basic = features_basic.withColumn(\n",
    "                \"usage_category\",\n",
    "                when(col(\"total_footfall\") < 100, \"Low\")\n",
    "                .when(col(\"total_footfall\") < 500, \"Medium\")\n",
    "                .otherwise(\"High\")\n",
    "            ).fillna({\"avg_aqi\": 0.0, \"total_footfall\": 0, \"avg_sentiment\": 0.0})\n",
    "            \n",
    "            print(\"   Created basic feature set\")\n",
    "            return features_basic\n",
    "            \n",
    "        except Exception as fallback_error:\n",
    "            print(f\" Fallback also failed: {str(fallback_error)}\")\n",
    "            # Return just the parks data if everything fails\n",
    "            return parks_df\n",
    "\n",
    "# Load and process data\n",
    "aq_df, foot_df, sent_df, parks_df = load_and_validate_data()\n",
    "features_df = create_comprehensive_features(aq_df, foot_df, sent_df, parks_df)\n",
    "\n",
    "\n",
    "# Create view and display sample\n",
    "features_df.createOrReplaceTempView(\"features_view\")\n",
    "print(\"\\n Feature Summary:\")\n",
    "features_df.select(\"park_id\", \"name\", \"city\", \"avg_aqi\", \"total_footfall\", \n",
    "                   \"avg_sentiment\", \"usage_category\", \"air_quality_category\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0230023f-3845-46c9-b6c9-6b138f9f684a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced Urban Green Space Management System\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    avg, sum, min, max, stddev, count, col, when, isnan, isnull, \n",
    "    date_format, hour, dayofweek, month, percentile_approx,\n",
    "    regexp_extract, split, size, desc, asc, lit\n",
    ")\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== URBAN GREEN SPACE MANAGEMENT SYSTEM ===\")\n",
    "print(\"Feature Engineering & Exploratory Data Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. ENHANCED FEATURE ENGINEERING & ENRICHMENT\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_validate_data():\n",
    "    \"\"\"Load datasets with validation and error handling\"\"\"\n",
    "    try:\n",
    "        print(\"\\n Loading datasets...\")\n",
    "        \n",
    "        # Load datasets\n",
    "        aq_df = spark.read.table(\"ml_project.bronze.air_quality\")\n",
    "        foot_df = spark.read.table(\"ml_project.bronze.footfall\")\n",
    "        sent_df = spark.read.table(\"ml_project.bronze.sentiment\")\n",
    "        parks_df = spark.read.table(\"ml_project.bronze.parks\")\n",
    "        \n",
    "        # Validate data\n",
    "        datasets = {\n",
    "            'Air Quality': aq_df,\n",
    "            'Footfall': foot_df,\n",
    "            'Sentiment': sent_df,\n",
    "            'Parks': parks_df\n",
    "        }\n",
    "        \n",
    "        for name, df in datasets.items():\n",
    "            count = df.count()\n",
    "            print(f\"   {name}: {count:,} records\")\n",
    "            \n",
    "            # Check for nulls\n",
    "            null_counts = []\n",
    "            for column in df.columns:\n",
    "                null_count = df.filter(col(column).isNull()).count()\n",
    "                if null_count > 0:\n",
    "                    null_counts.append(f\"{column}: {null_count}\")\n",
    "            \n",
    "            if null_counts:\n",
    "                print(f\"      Null values: {', '.join(null_counts)}\")\n",
    "        \n",
    "        return aq_df, foot_df, sent_df, parks_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def create_comprehensive_features(aq_df, foot_df, sent_df, parks_df):\n",
    "    \"\"\"Create comprehensive feature set with temporal and statistical features\"\"\"\n",
    "    \n",
    "    print(\"\\n Creating comprehensive features...\")\n",
    "    \n",
    "    try:\n",
    "        # Enhanced Air Quality Metrics\n",
    "        print(\"   Processing air quality metrics...\")\n",
    "        aqi_metrics = aq_df.groupBy(\"park_id\").agg(\n",
    "            avg(\"AQI\").alias(\"avg_aqi\"),\n",
    "            min(\"AQI\").alias(\"min_aqi\"),\n",
    "            max(\"AQI\").alias(\"max_aqi\"),\n",
    "            stddev(\"AQI\").alias(\"aqi_volatility\"),\n",
    "            percentile_approx(\"AQI\", 0.5).alias(\"median_aqi\"),\n",
    "            avg(\"no2_level\").alias(\"avg_no2\"),\n",
    "            avg(\"pm25_level\").alias(\"avg_pm25\"),\n",
    "            avg(\"o3_level\").alias(\"avg_o3\"),\n",
    "            count(\"*\").alias(\"aqi_measurements\")\n",
    "        )\n",
    "        \n",
    "        # Enhanced Footfall Metrics with Temporal Analysis\n",
    "        print(\"   Processing footfall metrics...\")\n",
    "        \n",
    "        # Check if timestamp column exists and try temporal features\n",
    "        footfall_columns = foot_df.columns\n",
    "        \n",
    "        if \"timestamp\" in footfall_columns:\n",
    "            try:\n",
    "                # Add temporal features to footfall data\n",
    "                foot_temporal = foot_df.withColumn(\"hour\", hour(col(\"timestamp\"))) \\\n",
    "                                      .withColumn(\"day_of_week\", dayofweek(col(\"timestamp\"))) \\\n",
    "                                      .withColumn(\"month\", month(col(\"timestamp\"))) \\\n",
    "                                      .withColumn(\"is_weekend\", when(col(\"day_of_week\").isin([1, 7]), 1).otherwise(0)) \\\n",
    "                                      .withColumn(\"is_event_day\", when(col(\"event_day\") == True, 1).otherwise(0))\n",
    "            except Exception as temporal_error:\n",
    "                print(f\"      Temporal features failed: {str(temporal_error)}\")\n",
    "                foot_temporal = foot_df.withColumn(\"is_weekend\", lit(0)) \\\n",
    "                                      .withColumn(\"is_event_day\", when(col(\"event_day\") == True, 1).otherwise(0))\n",
    "        else:\n",
    "            print(\"      No timestamp column found, using basic footfall metrics\")\n",
    "            foot_temporal = foot_df.withColumn(\"is_weekend\", lit(0)) \\\n",
    "                                  .withColumn(\"is_event_day\", when(col(\"event_day\") == True, 1).otherwise(0))\n",
    "        \n",
    "        foot_metrics = foot_temporal.groupBy(\"park_id\").agg(\n",
    "            sum(\"visitor_count\").alias(\"total_footfall\"),\n",
    "            avg(\"visitor_count\").alias(\"avg_footfall\"),\n",
    "            max(\"visitor_count\").alias(\"peak_footfall\"),\n",
    "            stddev(\"visitor_count\").alias(\"footfall_volatility\"),\n",
    "            sum(\"is_weekend\").alias(\"weekend_visits\"),\n",
    "            sum(\"is_event_day\").alias(\"event_days\"),\n",
    "            count(\"*\").alias(\"footfall_records\")\n",
    "        )\n",
    "        \n",
    "        # Peak hour analysis (if temporal features worked)\n",
    "        try:\n",
    "            if \"hour\" in foot_temporal.columns:\n",
    "                peak_hours = foot_temporal.groupBy(\"park_id\", \"hour\") \\\n",
    "                                         .agg(avg(\"visitor_count\").alias(\"avg_hourly_visitors\")) \\\n",
    "                                         .groupBy(\"park_id\") \\\n",
    "                                         .agg(max(\"avg_hourly_visitors\").alias(\"peak_hour_avg\"))\n",
    "                foot_metrics = foot_metrics.join(peak_hours, \"park_id\", \"left\")\n",
    "            else:\n",
    "                foot_metrics = foot_metrics.withColumn(\"peak_hour_avg\", lit(0.0))\n",
    "        except Exception as peak_error:\n",
    "            print(f\"      Peak hour analysis failed: {str(peak_error)}\")\n",
    "            foot_metrics = foot_metrics.withColumn(\"peak_hour_avg\", lit(0.0))\n",
    "        \n",
    "        # Enhanced Sentiment Metrics\n",
    "        print(\"   Processing sentiment metrics...\")\n",
    "        sent_metrics = sent_df.groupBy(\"park_id\").agg(\n",
    "            avg(\"sentiment_score\").alias(\"avg_sentiment\"),\n",
    "            min(\"sentiment_score\").alias(\"min_sentiment\"),\n",
    "            max(\"sentiment_score\").alias(\"max_sentiment\"),\n",
    "            stddev(\"sentiment_score\").alias(\"sentiment_volatility\"),\n",
    "            sum(when(col(\"sentiment_label\") == \"positive\", 1).otherwise(0)).alias(\"positive_count\"),\n",
    "            sum(when(col(\"sentiment_label\") == \"negative\", 1).otherwise(0)).alias(\"negative_count\"),\n",
    "            sum(when(col(\"sentiment_label\") == \"neutral\", 1).otherwise(0)).alias(\"neutral_count\"),\n",
    "            count(\"*\").alias(\"total_mentions\")\n",
    "        )\n",
    "        \n",
    "        # Calculate sentiment ratios\n",
    "        sent_metrics = sent_metrics.withColumn(\n",
    "            \"positive_ratio\", col(\"positive_count\") / col(\"total_mentions\")\n",
    "        ).withColumn(\n",
    "            \"negative_ratio\", col(\"negative_count\") / col(\"total_mentions\")\n",
    "        ).withColumn(\n",
    "            \"sentiment_polarity\", (col(\"positive_count\") - col(\"negative_count\")) / col(\"total_mentions\")\n",
    "        )\n",
    "        \n",
    "        # Merge all features\n",
    "        print(\"   Merging all features...\")\n",
    "        features_df = parks_df.join(aqi_metrics, \"park_id\", \"left\") \\\n",
    "                             .join(foot_metrics, \"park_id\", \"left\") \\\n",
    "                             .join(sent_metrics, \"park_id\", \"left\")\n",
    "        \n",
    "        # Create derived features\n",
    "        print(\"  • Creating derived features...\")\n",
    "        features_df = features_df.withColumn(\n",
    "            \"pollution_footfall_ratio\", \n",
    "            when(col(\"total_footfall\") > 0, col(\"avg_aqi\") / col(\"total_footfall\")).otherwise(0)\n",
    "        ).withColumn(\n",
    "            \"area_efficiency\", \n",
    "            when(col(\"area_sqm\") > 0, col(\"total_footfall\") / col(\"area_sqm\") * 1000).otherwise(0)  # visitors per 1000 sqm\n",
    "        ).withColumn(\n",
    "            \"sentiment_engagement_ratio\",\n",
    "            when(col(\"total_footfall\") > 0, col(\"total_mentions\") / col(\"total_footfall\") * 100).otherwise(0)\n",
    "        ).withColumn(\n",
    "            \"air_quality_category\",\n",
    "            when(col(\"avg_aqi\") <= 50, \"Good\")\n",
    "            .when(col(\"avg_aqi\") <= 100, \"Moderate\")\n",
    "            .when(col(\"avg_aqi\") <= 150, \"Unhealthy for Sensitive\")\n",
    "            .when(col(\"avg_aqi\") <= 200, \"Unhealthy\")\n",
    "            .otherwise(\"Very Unhealthy\")\n",
    "        ).withColumn(\n",
    "            \"usage_category\",\n",
    "            when(col(\"total_footfall\") < 100, \"Low\")\n",
    "            .when(col(\"total_footfall\") < 500, \"Medium\")\n",
    "            .when(col(\"total_footfall\") < 1000, \"High\")\n",
    "            .otherwise(\"Very High\")\n",
    "        ).withColumn(\n",
    "            \"park_size_category\",\n",
    "            when(col(\"area_sqm\") < 50000, \"Small\")\n",
    "            .when(col(\"area_sqm\") < 200000, \"Medium\")\n",
    "            .when(col(\"area_sqm\") < 500000, \"Large\")\n",
    "            .otherwise(\"Very Large\")\n",
    "        )\n",
    "        \n",
    "        # Fill null values with appropriate bronzes\n",
    "        fill_values = {\n",
    "            \"avg_aqi\": 0.0,\n",
    "            \"total_footfall\": 0,\n",
    "            \"avg_sentiment\": 0.0,\n",
    "            \"aqi_measurements\": 0,\n",
    "            \"footfall_records\": 0,\n",
    "            \"total_mentions\": 0,\n",
    "            \"min_aqi\": 0.0,\n",
    "            \"max_aqi\": 0.0,\n",
    "            \"aqi_volatility\": 0.0,\n",
    "            \"median_aqi\": 0.0,\n",
    "            \"avg_footfall\": 0.0,\n",
    "            \"peak_footfall\": 0,\n",
    "            \"footfall_volatility\": 0.0,\n",
    "            \"weekend_visits\": 0,\n",
    "            \"event_days\": 0,\n",
    "            \"peak_hour_avg\": 0.0,\n",
    "            \"positive_count\": 0,\n",
    "            \"negative_count\": 0,\n",
    "            \"neutral_count\": 0,\n",
    "            \"positive_ratio\": 0.0,\n",
    "            \"negative_ratio\": 0.0,\n",
    "            \"sentiment_polarity\": 0.0,\n",
    "            \"pollution_footfall_ratio\": 0.0,\n",
    "            \"area_efficiency\": 0.0,\n",
    "            \"sentiment_engagement_ratio\": 0.0\n",
    "        }\n",
    "        \n",
    "        features_df = features_df.fillna(fill_values)\n",
    "        \n",
    "        print(f\"   Created feature set with {len(features_df.columns)} columns\")\n",
    "        \n",
    "        return features_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error in feature engineering: {str(e)}\")\n",
    "        print(\" Attempting simplified feature engineering...\")\n",
    "        \n",
    "        # Fallback to basic feature engineering\n",
    "        try:\n",
    "            # Basic aggregations only\n",
    "            aqi_basic = aq_df.groupBy(\"park_id\").agg(avg(\"AQI\").alias(\"avg_aqi\"))\n",
    "            foot_basic = foot_df.groupBy(\"park_id\").agg(sum(\"visitor_count\").alias(\"total_footfall\"))\n",
    "            sent_basic = sent_df.groupBy(\"park_id\").agg(avg(\"sentiment_score\").alias(\"avg_sentiment\"))\n",
    "            \n",
    "            # Basic merge\n",
    "            features_basic = parks_df.join(aqi_basic, \"park_id\", \"left\") \\\n",
    "                                   .join(foot_basic, \"park_id\", \"left\") \\\n",
    "                                   .join(sent_basic, \"park_id\", \"left\")\n",
    "            \n",
    "            # Basic categories\n",
    "            features_basic = features_basic.withColumn(\n",
    "                \"usage_category\",\n",
    "                when(col(\"total_footfall\") < 100, \"Low\")\n",
    "                .when(col(\"total_footfall\") < 500, \"Medium\")\n",
    "                .otherwise(\"High\")\n",
    "            ).fillna({\"avg_aqi\": 0.0, \"total_footfall\": 0, \"avg_sentiment\": 0.0})\n",
    "            \n",
    "            print(\"   Created basic feature set\")\n",
    "            return features_basic\n",
    "            \n",
    "        except Exception as fallback_error:\n",
    "            print(f\" Fallback also failed: {str(fallback_error)}\")\n",
    "            # Return just the parks data if everything fails\n",
    "            return parks_df\n",
    "\n",
    "# Load and process data\n",
    "aq_df, foot_df, sent_df, parks_df = load_and_validate_data()\n",
    "features_df = create_comprehensive_features(aq_df, foot_df, sent_df, parks_df)\n",
    "\n",
    "\n",
    "# Create view and display sample\n",
    "features_df.createOrReplaceTempView(\"features_view\")\n",
    "print(\"\\n Feature Summary:\")\n",
    "features_df.select(\"park_id\", \"name\", \"city\", \"avg_aqi\", \"total_footfall\", \n",
    "                   \"avg_sentiment\", \"usage_category\", \"air_quality_category\").show(10, truncate=False)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ENHANCED EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n Starting Exploratory Data Analysis...\")\n",
    "\n",
    "# Set up matplotlib for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def create_comprehensive_eda(features_df):\n",
    "    \"\"\"Create comprehensive EDA with multiple visualizations\"\"\"\n",
    "    \n",
    "    # Convert to Pandas for plotting - handle potential errors\n",
    "    try:\n",
    "        features_pd = features_df.toPandas()\n",
    "    except Exception as e:\n",
    "        print(f\" Error converting to Pandas: {str(e)}\")\n",
    "        print(\"Attempting with limited columns...\")\n",
    "        \n",
    "        # Try with essential columns only\n",
    "        essential_cols = [\"park_id\", \"name\", \"city\", \"avg_aqi\", \"total_footfall\", \n",
    "                         \"avg_sentiment\", \"area_sqm\", \"usage_category\"]\n",
    "        available_cols = [col for col in essential_cols if col in features_df.columns]\n",
    "        features_pd = features_df.select(*available_cols).toPandas()\n",
    "    \n",
    "    print(f\"   Converted {len(features_pd)} records to Pandas DataFrame\")\n",
    "    \n",
    "    # Handle missing columns gracefully\n",
    "    required_columns = {\n",
    "        'avg_aqi': 0.0,\n",
    "        'total_footfall': 0,\n",
    "        'avg_sentiment': 0.0,\n",
    "        'area_sqm': 0.0,\n",
    "        'usage_category': 'Unknown',\n",
    "        'air_quality_category': 'Unknown'\n",
    "    }\n",
    "    \n",
    "    for col, bronze_val in required_columns.items():\n",
    "        if col not in features_pd.columns:\n",
    "            features_pd[col] = bronze_val\n",
    "            print(f\"    Added missing column '{col}' with bronze value\")\n",
    "    \n",
    "    # Remove any infinite or extremely large values\n",
    "    numeric_cols = features_pd.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        features_pd[col] = features_pd[col].replace([np.inf, -np.inf], np.nan)\n",
    "        if features_pd[col].dtype in ['float64', 'float32']:\n",
    "            features_pd[col] = features_pd[col].fillna(features_pd[col].median())\n",
    "        else:\n",
    "            features_pd[col] = features_pd[col].fillna(0)\n",
    "    \n",
    "    # Create comprehensive plots\n",
    "    fig = plt.figure(figsize=(20, 24))\n",
    "    \n",
    "    # 1. Air Quality vs Footfall Analysis\n",
    "    plt.subplot(4, 3, 1)\n",
    "    valid_data = features_pd[(features_pd['avg_aqi'] > 0) & (features_pd['total_footfall'] > 0)]\n",
    "    plt.scatter(valid_data['avg_aqi'], valid_data['total_footfall'], \n",
    "                alpha=0.6, s=50, c=valid_data['avg_sentiment'], cmap='RdYlGn')\n",
    "    plt.xlabel('Average AQI')\n",
    "    plt.ylabel('Total Footfall')\n",
    "    plt.title('Air Quality vs Footfall\\n(Color = Sentiment)')\n",
    "    plt.colorbar(label='Avg Sentiment')\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    if len(valid_data) > 1:\n",
    "        corr = valid_data['avg_aqi'].corr(valid_data['total_footfall'])\n",
    "        plt.text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=plt.gca().transAxes, \n",
    "                bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # 2. Usage Category Distribution\n",
    "    plt.subplot(4, 3, 2)\n",
    "    usage_counts = features_pd['usage_category'].value_counts()\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(usage_counts)))\n",
    "    bars = plt.bar(usage_counts.index, usage_counts.values, color=colors)\n",
    "    plt.title('Park Usage Category Distribution')\n",
    "    plt.xlabel('Usage Category')\n",
    "    plt.ylabel('Number of Parks')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{int(height)}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Air Quality Category Distribution\n",
    "    plt.subplot(4, 3, 3)\n",
    "    if 'air_quality_category' in features_pd.columns:\n",
    "        aqi_counts = features_pd['air_quality_category'].value_counts()\n",
    "        colors = ['green', 'yellow', 'orange', 'red', 'purple'][:len(aqi_counts)]\n",
    "        plt.pie(aqi_counts.values, labels=aqi_counts.index, autopct='%1.1f%%', \n",
    "                colors=colors, startangle=90)\n",
    "        plt.title('Air Quality Distribution')\n",
    "    \n",
    "    # 4. Sentiment Analysis\n",
    "    plt.subplot(4, 3, 4)\n",
    "    sentiment_data = features_pd[features_pd['avg_sentiment'].notna()]\n",
    "    if len(sentiment_data) > 0:\n",
    "        plt.hist(sentiment_data['avg_sentiment'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        plt.axvline(sentiment_data['avg_sentiment'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {sentiment_data[\"avg_sentiment\"].mean():.3f}')\n",
    "        plt.xlabel('Average Sentiment Score')\n",
    "        plt.ylabel('Number of Parks')\n",
    "        plt.title('Sentiment Score Distribution')\n",
    "        plt.legend()\n",
    "    \n",
    "    # 5. Park Size vs Usage\n",
    "    plt.subplot(4, 3, 5)\n",
    "    size_usage = features_pd[(features_pd['area_sqm'] > 0) & (features_pd['total_footfall'] > 0)]\n",
    "    if len(size_usage) > 0:\n",
    "        plt.scatter(size_usage['area_sqm']/1000, size_usage['total_footfall'], \n",
    "                   alpha=0.6, s=50)\n",
    "        plt.xlabel('Park Area (1000 sqm)')\n",
    "        plt.ylabel('Total Footfall')\n",
    "        plt.title('Park Size vs Usage')\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(size_usage) > 1:\n",
    "            z = np.polyfit(np.log(size_usage['area_sqm']), np.log(size_usage['total_footfall']), 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_trend = np.logspace(np.log10(size_usage['area_sqm'].min()), \n",
    "                                np.log10(size_usage['area_sqm'].max()), 100)\n",
    "            y_trend = np.exp(p(np.log(x_trend)))\n",
    "            plt.plot(x_trend/1000, y_trend, \"r--\", alpha=0.8, label='Trend')\n",
    "            plt.legend()\n",
    "    \n",
    "    # 6. City-wise Analysis\n",
    "    plt.subplot(4, 3, 6)\n",
    "    city_stats = features_pd.groupby('city').agg({\n",
    "        'total_footfall': 'sum',\n",
    "        'avg_aqi': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    if len(city_stats) > 0:\n",
    "        city_stats = city_stats.sort_values('total_footfall', ascending=True).tail(10)\n",
    "        bars = plt.barh(range(len(city_stats)), city_stats['total_footfall'])\n",
    "        plt.yticks(range(len(city_stats)), city_stats['city'])\n",
    "        plt.xlabel('Total Footfall')\n",
    "        plt.title('Top 10 Cities by Total Footfall')\n",
    "        \n",
    "        # Color bars by AQI\n",
    "        for i, (bar, aqi) in enumerate(zip(bars, city_stats['avg_aqi'])):\n",
    "            if not np.isnan(aqi):\n",
    "                color = 'green' if aqi < 50 else 'yellow' if aqi < 100 else 'red'\n",
    "                bar.set_color(color)\n",
    "    \n",
    "    # 7. Correlation Heatmap\n",
    "    plt.subplot(4, 3, 7)\n",
    "    numeric_features = ['avg_aqi', 'total_footfall', 'avg_sentiment', 'area_sqm', \n",
    "                       'area_efficiency', 'pollution_footfall_ratio']\n",
    "    correlation_data = features_pd[numeric_features].select_dtypes(include=[np.number])\n",
    "    \n",
    "    if len(correlation_data.columns) > 1:\n",
    "        correlation_matrix = correlation_data.corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                   square=True, fmt='.2f')\n",
    "        plt.title('Feature Correlation Matrix')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "    \n",
    "    # 8. Pollution vs Sentiment\n",
    "    plt.subplot(4, 3, 8)\n",
    "    pollution_sentiment = features_pd[(features_pd['avg_aqi'] > 0) & (features_pd['avg_sentiment'].notna())]\n",
    "    if len(pollution_sentiment) > 0:\n",
    "        plt.scatter(pollution_sentiment['avg_aqi'], pollution_sentiment['avg_sentiment'], \n",
    "                   alpha=0.6, s=50)\n",
    "        plt.xlabel('Average AQI')\n",
    "        plt.ylabel('Average Sentiment')\n",
    "        plt.title('Air Quality vs Public Sentiment')\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(pollution_sentiment) > 1:\n",
    "            z = np.polyfit(pollution_sentiment['avg_aqi'], pollution_sentiment['avg_sentiment'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(pollution_sentiment['avg_aqi'], p(pollution_sentiment['avg_aqi']), \n",
    "                    \"r--\", alpha=0.8)\n",
    "            \n",
    "            corr = pollution_sentiment['avg_aqi'].corr(pollution_sentiment['avg_sentiment'])\n",
    "            plt.text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=plt.gca().transAxes,\n",
    "                    bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # 9. Weekend vs Weekday Usage\n",
    "    plt.subplot(4, 3, 9)\n",
    "    if 'weekend_visits' in features_pd.columns and 'footfall_records' in features_pd.columns:\n",
    "        valid_weekend_data = features_pd[\n",
    "            (features_pd['weekend_visits'] >= 0) & \n",
    "            (features_pd['footfall_records'] > 0) &\n",
    "            (features_pd['weekend_visits'].notna()) &\n",
    "            (features_pd['footfall_records'].notna())\n",
    "        ].copy()\n",
    "        \n",
    "        if len(valid_weekend_data) > 0:\n",
    "            valid_weekend_data['weekend_ratio'] = (\n",
    "                valid_weekend_data['weekend_visits'] / valid_weekend_data['footfall_records']\n",
    "            )\n",
    "            # Remove any invalid ratios\n",
    "            valid_weekend_data = valid_weekend_data[\n",
    "                (valid_weekend_data['weekend_ratio'] >= 0) & \n",
    "                (valid_weekend_data['weekend_ratio'] <= 1)\n",
    "            ]\n",
    "            \n",
    "            if len(valid_weekend_data) > 0:\n",
    "                plt.hist(valid_weekend_data['weekend_ratio'], bins=15, alpha=0.7, \n",
    "                        color='lightcoral', edgecolor='black')\n",
    "                plt.xlabel('Weekend Visit Ratio')\n",
    "                plt.ylabel('Number of Parks')\n",
    "                plt.title('Weekend vs Weekday Usage Pattern')\n",
    "                mean_ratio = valid_weekend_data['weekend_ratio'].mean()\n",
    "                plt.axvline(mean_ratio, color='red', \n",
    "                           linestyle='--', label=f'Mean: {mean_ratio:.2f}')\n",
    "                plt.legend()\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'No valid weekend data', ha='center', va='center', \n",
    "                        transform=plt.gca().transAxes)\n",
    "                plt.title('Weekend vs Weekday Usage Pattern')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No weekend data available', ha='center', va='center', \n",
    "                    transform=plt.gca().transAxes)\n",
    "            plt.title('Weekend vs Weekday Usage Pattern')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Weekend data not available', ha='center', va='center', \n",
    "                transform=plt.gca().transAxes)\n",
    "        plt.title('Weekend vs Weekday Usage Pattern')\n",
    "    \n",
    "    # 10. Area Efficiency Distribution\n",
    "    plt.subplot(4, 3, 10)\n",
    "    if 'area_efficiency' in features_pd.columns:\n",
    "        efficiency_data = features_pd[\n",
    "            (features_pd['area_efficiency'] > 0) & \n",
    "            (features_pd['area_efficiency'].notna()) &\n",
    "            (features_pd['area_efficiency'] < features_pd['area_efficiency'].quantile(0.95))  # Remove outliers\n",
    "        ]\n",
    "        if len(efficiency_data) > 0:\n",
    "            plt.hist(efficiency_data['area_efficiency'], bins=20, alpha=0.7, \n",
    "                    color='lightgreen', edgecolor='black')\n",
    "            plt.xlabel('Area Efficiency (visitors per 1000 sqm)')\n",
    "            plt.ylabel('Number of Parks')\n",
    "            plt.title('Park Area Efficiency Distribution')\n",
    "            mean_eff = efficiency_data['area_efficiency'].mean()\n",
    "            plt.axvline(mean_eff, color='red', \n",
    "                       linestyle='--', label=f'Mean: {mean_eff:.2f}')\n",
    "            plt.legend()\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No efficiency data available', ha='center', va='center', \n",
    "                    transform=plt.gca().transAxes)\n",
    "            plt.title('Park Area Efficiency Distribution')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Area efficiency not calculated', ha='center', va='center', \n",
    "                transform=plt.gca().transAxes)\n",
    "        plt.title('Park Area Efficiency Distribution')\n",
    "    \n",
    "    # 11. Top Performing Parks\n",
    "    plt.subplot(4, 3, 11)\n",
    "    top_parks = features_pd.nlargest(10, 'total_footfall')[['name', 'total_footfall', 'city']]\n",
    "    if len(top_parks) > 0:\n",
    "        bars = plt.barh(range(len(top_parks)), top_parks['total_footfall'])\n",
    "        plt.yticks(range(len(top_parks)), \n",
    "                  [f\"{name[:20]}... ({city})\" if len(name) > 20 else f\"{name} ({city})\" \n",
    "                   for name, city in zip(top_parks['name'], top_parks['city'])])\n",
    "        plt.xlabel('Total Footfall')\n",
    "        plt.title('Top 10 Parks by Footfall')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, value) in enumerate(zip(bars, top_parks['total_footfall'])):\n",
    "            plt.text(value + max(top_parks['total_footfall']) * 0.01, i, \n",
    "                    f'{int(value):,}', va='center', fontsize=8)\n",
    "    \n",
    "    # 12. Sentiment Engagement Analysis\n",
    "    plt.subplot(4, 3, 12)\n",
    "    if 'sentiment_engagement_ratio' in features_pd.columns:\n",
    "        engagement_data = features_pd[\n",
    "            (features_pd['sentiment_engagement_ratio'] > 0) &\n",
    "            (features_pd['sentiment_engagement_ratio'].notna()) &\n",
    "            (features_pd['total_footfall'] > 0) &\n",
    "            (features_pd['avg_sentiment'].notna())\n",
    "        ]\n",
    "        if len(engagement_data) > 0:\n",
    "            plt.scatter(engagement_data['total_footfall'], \n",
    "                       engagement_data['sentiment_engagement_ratio'], \n",
    "                       alpha=0.6, s=50, c=engagement_data['avg_sentiment'], cmap='RdYlGn')\n",
    "            plt.xlabel('Total Footfall')\n",
    "            plt.ylabel('Sentiment Engagement Ratio (%)')\n",
    "            plt.title('Footfall vs Social Media Engagement')\n",
    "            plt.colorbar(label='Avg Sentiment')\n",
    "            if engagement_data['total_footfall'].min() > 0:\n",
    "                plt.xscale('log')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No engagement data available', ha='center', va='center', \n",
    "                    transform=plt.gca().transAxes)\n",
    "            plt.title('Footfall vs Social Media Engagement')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Engagement ratio not calculated', ha='center', va='center', \n",
    "                transform=plt.gca().transAxes)\n",
    "        plt.title('Footfall vs Social Media Engagement')\n",
    "    \n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.show()\n",
    "    \n",
    "    return features_pd\n",
    "\n",
    "# Generate comprehensive EDA with error handling\n",
    "try:\n",
    "    print(\"\\n Starting Exploratory Data Analysis...\")\n",
    "    features_pd = create_comprehensive_eda(features_df)\n",
    "except Exception as eda_error:\n",
    "    print(f\" EDA failed with error: {str(eda_error)}\")\n",
    "    print(\" Attempting basic EDA...\")\n",
    "    \n",
    "    # Fallback to basic EDA\n",
    "    try:\n",
    "        # Simple conversion to pandas with basic columns only\n",
    "        basic_columns = [\"park_id\", \"name\", \"city\"]\n",
    "        \n",
    "        # Add available numeric columns\n",
    "        all_columns = features_df.columns\n",
    "        for col in [\"avg_aqi\", \"total_footfall\", \"avg_sentiment\", \"area_sqm\"]:\n",
    "            if col in all_columns:\n",
    "                basic_columns.append(col)\n",
    "        \n",
    "        features_pd = features_df.select(*basic_columns).toPandas()\n",
    "        \n",
    "        # Create simple visualizations\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # Plot 1: Simple histogram of footfall (if available)\n",
    "        if 'total_footfall' in features_pd.columns:\n",
    "            footfall_data = features_pd[features_pd['total_footfall'] > 0]['total_footfall']\n",
    "            if len(footfall_data) > 0:\n",
    "                axes[0, 0].hist(footfall_data, bins=20, alpha=0.7)\n",
    "                axes[0, 0].set_title('Footfall Distribution')\n",
    "                axes[0, 0].set_xlabel('Total Footfall')\n",
    "                axes[0, 0].set_ylabel('Number of Parks')\n",
    "        \n",
    "        # Plot 2: Simple histogram of AQI (if available)\n",
    "        if 'avg_aqi' in features_pd.columns:\n",
    "            aqi_data = features_pd[features_pd['avg_aqi'] > 0]['avg_aqi']\n",
    "            if len(aqi_data) > 0:\n",
    "                axes[0, 1].hist(aqi_data, bins=20, alpha=0.7)\n",
    "                axes[0, 1].set_title('Air Quality Distribution')\n",
    "                axes[0, 1].set_xlabel('Average AQI')\n",
    "                axes[0, 1].set_ylabel('Number of Parks')\n",
    "        \n",
    "        # Plot 3: Simple histogram of sentiment (if available)\n",
    "        if 'avg_sentiment' in features_pd.columns:\n",
    "            sent_data = features_pd[features_pd['avg_sentiment'].notna()]['avg_sentiment']\n",
    "            if len(sent_data) > 0:\n",
    "                axes[1, 0].hist(sent_data, bins=20, alpha=0.7)\n",
    "                axes[1, 0].set_title('Sentiment Distribution')\n",
    "                axes[1, 0].set_xlabel('Average Sentiment')\n",
    "                axes[1, 0].set_ylabel('Number of Parks')\n",
    "        \n",
    "        # Plot 4: Simple scatter plot (if data available)\n",
    "        if 'avg_aqi' in features_pd.columns and 'total_footfall' in features_pd.columns:\n",
    "            valid_data = features_pd[(features_pd['avg_aqi'] > 0) & (features_pd['total_footfall'] > 0)]\n",
    "            if len(valid_data) > 0:\n",
    "                axes[1, 1].scatter(valid_data['avg_aqi'], valid_data['total_footfall'], alpha=0.6)\n",
    "                axes[1, 1].set_title('AQI vs Footfall')\n",
    "                axes[1, 1].set_xlabel('Average AQI')\n",
    "                axes[1, 1].set_ylabel('Total Footfall')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\" Basic EDA completed successfully\")\n",
    "        \n",
    "    except Exception as basic_eda_error:\n",
    "        print(f\" Basic EDA also failed: {str(basic_eda_error)}\")\n",
    "        print(\"  Skipping visualization, proceeding with analysis...\")\n",
    "        \n",
    "        # Just get basic pandas dataframe for summary stats\n",
    "        try:\n",
    "            features_pd = features_df.select(\"park_id\", \"name\", \"city\").limit(100).toPandas()\n",
    "        except:\n",
    "            features_pd = pd.DataFrame()  # Empty dataframe as last resort\n",
    "\n",
    "# =============================================================================\n",
    "# 4. SUMMARY STATISTICS AND INSIGHTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n SUMMARY STATISTICS AND INSIGHTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def generate_insights(features_pd, features_df=None):\n",
    "    \"\"\"Generate actionable insights from the analysis\"\"\"\n",
    "    \n",
    "    print(f\"📈 Dataset Overview:\")\n",
    "    \n",
    "    # Handle case where features_pd might be empty or have limited columns\n",
    "    if len(features_pd) == 0:\n",
    "        print(\"    No data available for analysis\")\n",
    "        if features_df is not None:\n",
    "            try:\n",
    "                total_parks = features_df.count()\n",
    "                print(f\"  • Total Parks (from Spark DF): {total_parks:,}\")\n",
    "            except:\n",
    "                print(\"  • Unable to get park count\")\n",
    "        return []\n",
    "    \n",
    "    total_parks = len(features_pd)\n",
    "    print(f\"   Total Parks: {total_parks:,}\")\n",
    "    \n",
    "    # Basic statistics with error handling\n",
    "    numeric_columns = features_pd.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in ['total_footfall', 'avg_aqi', 'avg_sentiment']:\n",
    "        if col in features_pd.columns:\n",
    "            try:\n",
    "                total_val = features_pd[col].sum() if col == 'total_footfall' else features_pd[col].mean()\n",
    "                if col == 'total_footfall':\n",
    "                    print(f\"  • Total Visitors: {total_val:,.0f}\")\n",
    "                elif col == 'avg_aqi':\n",
    "                    print(f\"  • Average AQI: {total_val:.1f}\")\n",
    "                elif col == 'avg_sentiment':\n",
    "                    print(f\"  • Average Sentiment: {total_val:.3f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Could not calculate {col}: {str(e)}\")\n",
    "    \n",
    "    # Usage patterns with error handling\n",
    "    if 'usage_category' in features_pd.columns:\n",
    "        try:\n",
    "            usage_breakdown = features_pd['usage_category'].value_counts()\n",
    "            print(f\"\\n Usage Patterns:\")\n",
    "            for category, count in usage_breakdown.items():\n",
    "                percentage = (count/total_parks)*100\n",
    "                print(f\"  • {category} Usage: {count} parks ({percentage:.1f}%)\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n Usage Patterns: Unable to calculate - {str(e)}\")\n",
    "    \n",
    "    # Air quality breakdown with error handling\n",
    "    if 'air_quality_category' in features_pd.columns:\n",
    "        try:\n",
    "            aqi_breakdown = features_pd['air_quality_category'].value_counts()\n",
    "            print(f\"\\n Air Quality Breakdown:\")\n",
    "            for category, count in aqi_breakdown.items():\n",
    "                percentage = (count/total_parks)*100\n",
    "                print(f\"  • {category}: {count} parks ({percentage:.1f}%)\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n Air Quality: Unable to calculate - {str(e)}\")\n",
    "    \n",
    "    # Top performers with error handling\n",
    "    if 'total_footfall' in features_pd.columns and 'name' in features_pd.columns:\n",
    "        try:\n",
    "            top_footfall = features_pd.nlargest(3, 'total_footfall')[['name', 'city', 'total_footfall']]\n",
    "            if len(top_footfall) > 0:\n",
    "                print(f\"\\n Top Parks by Footfall:\")\n",
    "                for idx, (_, row) in enumerate(top_footfall.iterrows(), 1):\n",
    "                    city_info = f\" ({row['city']})\" if 'city' in row and pd.notna(row['city']) else \"\"\n",
    "                    print(f\"  {idx}. {row['name']}{city_info}: {row['total_footfall']:,.0f} visitors\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n Top Parks: Unable to calculate - {str(e)}\")\n",
    "    \n",
    "    # Simple correlations with error handling\n",
    "    try:\n",
    "        correlation_insights = []\n",
    "        \n",
    "        if len(numeric_columns) >= 2:\n",
    "            # Try basic correlations\n",
    "            for col1, col2 in [('avg_aqi', 'total_footfall'), ('avg_aqi', 'avg_sentiment'), ('area_sqm', 'total_footfall')]:\n",
    "                if col1 in features_pd.columns and col2 in features_pd.columns:\n",
    "                    valid_data = features_pd[[col1, col2]].dropna()\n",
    "                    if len(valid_data) > 1:\n",
    "                        corr = valid_data[col1].corr(valid_data[col2])\n",
    "                        if not np.isnan(corr):\n",
    "                            correlation_insights.append(f\"  • {col1.replace('_', ' ').title()} vs {col2.replace('_', ' ').title()}: {corr:.3f}\")\n",
    "        \n",
    "        if correlation_insights:\n",
    "            print(f\"\\n🔗 Key Correlations:\")\n",
    "            for insight in correlation_insights:\n",
    "                print(insight)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n🔗 Correlations: Unable to calculate - {str(e)}\")\n",
    "    \n",
    "    # Basic recommendations\n",
    "    print(f\"\\n Basic Recommendations:\")\n",
    "    \n",
    "    try:\n",
    "        # Poor air quality parks\n",
    "        if 'avg_aqi' in features_pd.columns:\n",
    "            poor_aqi_parks = features_pd[features_pd['avg_aqi'] > 100]\n",
    "            if len(poor_aqi_parks) > 0:\n",
    "                print(f\"  • {len(poor_aqi_parks)} parks have poor air quality (AQI > 100) - consider air purification measures\")\n",
    "        \n",
    "        # Check for data availability\n",
    "        available_metrics = [col for col in ['avg_aqi', 'total_footfall', 'avg_sentiment'] if col in features_pd.columns]\n",
    "        if len(available_metrics) > 0:\n",
    "            print(f\"  • Continue analysis with available metrics: {', '.join(available_metrics)}\")\n",
    "        else:\n",
    "            print(f\"  • Limited data available - focus on data collection and validation\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"    Could not generate specific recommendations: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n Basic analysis completed!\")\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Generate final insights with error handling\n",
    "try:\n",
    "    insights = generate_insights(features_pd, features_df)\n",
    "except Exception as insights_error:\n",
    "    print(f\" Insights generation failed: {str(insights_error)}\")\n",
    "    print(\" Attempting basic dataset summary...\")\n",
    "    \n",
    "    try:\n",
    "        park_count = features_df.count()\n",
    "        print(f\" Successfully processed {park_count} parks\")\n",
    "    except:\n",
    "        print(\"  Unable to get basic statistics\")\n",
    "\n",
    "# Save processed features for next steps\n",
    "print(f\"\\n Saving processed features...\")\n",
    "features_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"ml_project.silver.processed_features\")\n",
    "\n",
    "print(\" Features saved to 'ml_project.silver.processed_features' table\")\n",
    "\n",
    "display(features_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bb86aec-0ce5-4ab4-a66d-d442bf4ab55b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"ml_project.silver.processed_features\")\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
