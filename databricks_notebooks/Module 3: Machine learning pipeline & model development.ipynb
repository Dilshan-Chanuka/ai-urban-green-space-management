{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2a0d27b-0235-4166-8a1f-9f1796ac60db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"ml_project.bronze.sentiment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "411c536a-3912-455e-afbe-5706278723c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODULE 3: MACHINE LEARNING PIPELINE & MODEL DEVELOPMENT\n",
    "# Urban Green Space Management System \n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, auc\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== URBAN GREEN SPACE MANAGEMENT SYSTEM ===\")\n",
    "print(\"Module 3: Machine Learning Pipeline & Model Development (Improved)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. INITIALIZATION AND CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize Spark with optimized settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UGSM_ML_Pipeline_Improved\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configuration\n",
    "CATALOG = \"ml_project\"\n",
    "SILVER_SCHEMA = \"silver\"\n",
    "GOLD_SCHEMA = \"gold\"\n",
    "\n",
    "# MLflow configuration\n",
    "mlflow.set_registry_uri(\"databricks\")\n",
    "experiment_name = \"/Users/dilshanchanuka.bc@gmail.com/Urban Green Space Management\"\n",
    "\n",
    "try:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(f\" MLflow experiment set: {experiment_name}\")\n",
    "except Exception as e:\n",
    "    print(f\" MLflow experiment setup: {str(e)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA LOADING AND PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_prepare_ml_data():\n",
    "    \"\"\"Load processed features and prepare for ML with comprehensive error handling\"\"\"\n",
    "    \n",
    "    print(\"\\n📊 Loading processed features for ML...\")\n",
    "    \n",
    "    try:\n",
    "        # Load processed features\n",
    "        df_spark = spark.read.table(f\"{CATALOG}.{SILVER_SCHEMA}.processed_features\")\n",
    "        print(f\"  ✅ Loaded features from {CATALOG}.{SILVER_SCHEMA}.processed_features\")\n",
    "        \n",
    "        # Convert to Pandas for sklearn\n",
    "        df = df_spark.toPandas()\n",
    "        print(f\"  ✅ Converted to Pandas: {len(df)} records, {len(df.columns)} columns\")\n",
    "        \n",
    "        # Validate essential columns exist\n",
    "        required_cols = ['avg_aqi', 'avg_sentiment']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"  ⚠️  Missing required columns: {missing_cols}\")\n",
    "            # Add missing columns with default values\n",
    "            for col in missing_cols:\n",
    "                if col == 'avg_aqi':\n",
    "                    df[col] = 75.0  # Default AQI\n",
    "                elif col == 'avg_sentiment':\n",
    "                    df[col] = 0.0   # Neutral sentiment\n",
    "            print(f\"  ✅ Added missing columns with default values\")\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error loading features: {str(e)}\")\n",
    "\n",
    "def create_target_variable(df):\n",
    "    \"\"\"Create intervention required target variable with business logic\"\"\"\n",
    "    \n",
    "    print(\"\\n🎯 Creating target variable...\")\n",
    "    \n",
    "    # Enhanced business logic for intervention requirement\n",
    "    # Parks need intervention if:\n",
    "    # 1. High pollution (AQI > 75) AND negative sentiment (< 0), OR\n",
    "    # 2. Very high pollution (AQI > 100), OR  \n",
    "    # 3. Very negative sentiment (< -0.3) AND moderate usage\n",
    "    \n",
    "    intervention_conditions = (\n",
    "        ((df[\"avg_aqi\"] > 75) & (df[\"avg_sentiment\"] < 0)) |\n",
    "        (df[\"avg_aqi\"] > 100) |\n",
    "        ((df[\"avg_sentiment\"] < -0.3) & (df.get(\"total_footfall\", 0) > 200))\n",
    "    )\n",
    "    \n",
    "    df[\"intervention_required\"] = intervention_conditions.astype(int)\n",
    "    \n",
    "    # Print target distribution\n",
    "    target_dist = df[\"intervention_required\"].value_counts()\n",
    "    total = len(df)\n",
    "    \n",
    "    print(f\"  📊 Target Variable Distribution:\")\n",
    "    print(f\"    • No Intervention (0): {target_dist.get(0, 0)} parks ({target_dist.get(0, 0)/total*100:.1f}%)\")\n",
    "    print(f\"    • Intervention Required (1): {target_dist.get(1, 0)} parks ({target_dist.get(1, 0)/total*100:.1f}%)\")\n",
    "    \n",
    "    if target_dist.get(1, 0) == 0:\n",
    "        print(\"  ⚠️  No positive cases found, adjusting thresholds...\")\n",
    "        # More lenient criteria\n",
    "        df[\"intervention_required\"] = ((df[\"avg_aqi\"] > 70) | (df[\"avg_sentiment\"] < 0.1)).astype(int)\n",
    "        target_dist = df[\"intervention_required\"].value_counts()\n",
    "        print(f\"    • Adjusted - Intervention Required: {target_dist.get(1, 0)} parks\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_features_for_ml(df):\n",
    "    \"\"\"Prepare features for machine learning with proper handling\"\"\"\n",
    "    \n",
    "    print(\"\\n🔧 Preparing features for ML...\")\n",
    "    \n",
    "    # Define feature categories\n",
    "    exclude_cols = {\n",
    "        \"park_id\", \"name\", \"city\", \"air_quality_category\",\n",
    "        \"park_size_category\", \"usage_category\", \"intervention_required\"\n",
    "    }\n",
    "    \n",
    "    # Get all potential feature columns\n",
    "    all_cols = set(df.columns)\n",
    "    feature_cols = list(all_cols - exclude_cols)\n",
    "    \n",
    "    # Filter to numeric columns only\n",
    "    numeric_features = []\n",
    "    for col in feature_cols:\n",
    "        if df[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "            numeric_features.append(col)\n",
    "    \n",
    "    print(f\"  📋 Selected {len(numeric_features)} numeric features:\")\n",
    "    for i, feat in enumerate(numeric_features, 1):\n",
    "        print(f\"    {i:2d}. {feat}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = df[numeric_features].copy()\n",
    "    \n",
    "    # Fill missing values with median for numeric columns\n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            median_val = X[col].median()\n",
    "            X[col] = X[col].fillna(median_val)\n",
    "            print(f\"  🔧 Filled {X[col].isnull().sum()} missing values in {col} with median: {median_val:.2f}\")\n",
    "    \n",
    "    # Handle infinite values\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    y = df[\"intervention_required\"]\n",
    "    \n",
    "    print(f\"  ✅ Feature matrix shape: {X.shape}\")\n",
    "    print(f\"  ✅ Target vector shape: {y.shape}\")\n",
    "    \n",
    "    return X, y, numeric_features\n",
    "\n",
    "# =============================================================================\n",
    "# 3. MODEL DEVELOPMENT AND TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "def create_ml_pipelines():\n",
    "    \"\"\"Create multiple ML pipelines for comparison\"\"\"\n",
    "    \n",
    "    print(\"\\n🤖 Creating ML pipelines...\")\n",
    "    \n",
    "    pipelines = {\n",
    "        'logistic_regression': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "        ]),\n",
    "        \n",
    "        'random_forest': Pipeline([\n",
    "            ('scaler', RobustScaler()),  # RF doesn't need scaling but helps with consistency\n",
    "            ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "        ]),\n",
    "        \n",
    "        'gradient_boosting': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    # Hyperparameter grids\n",
    "    param_grids = {\n",
    "        'logistic_regression': {\n",
    "            'classifier__C': [0.1, 1.0, 10.0],\n",
    "            'classifier__penalty': ['l1', 'l2'],\n",
    "            'classifier__solver': ['liblinear']\n",
    "        },\n",
    "        \n",
    "        'random_forest': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__max_depth': [5, 10, None],\n",
    "            'classifier__min_samples_split': [2, 5, 10]\n",
    "        },\n",
    "        \n",
    "        'gradient_boosting': {\n",
    "            'classifier__n_estimators': [50, 100],\n",
    "            'classifier__learning_rate': [0.1, 0.2],\n",
    "            'classifier__max_depth': [3, 5]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"  ✅ Created {len(pipelines)} ML pipelines\")\n",
    "    return pipelines, param_grids\n",
    "\n",
    "def evaluate_model_performance(model, X_test, y_test, model_name):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    \n",
    "    print(f\"\\n📊 Evaluating {model_name} Performance...\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, average='binary', zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, average='binary', zero_division=0),\n",
    "        'f1': f1_score(y_test, y_pred, average='binary', zero_division=0)\n",
    "    }\n",
    "    \n",
    "    if y_pred_proba is not None and len(np.unique(y_test)) > 1:\n",
    "        try:\n",
    "            metrics['auc_roc'] = roc_auc_score(y_test, y_pred_proba)\n",
    "        except:\n",
    "            metrics['auc_roc'] = 0.5\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"  📈 Performance Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"    • {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"  📊 Confusion Matrix:\")\n",
    "    print(f\"    • True Negatives: {cm[0,0]}\")\n",
    "    print(f\"    • False Positives: {cm[0,1]}\")\n",
    "    print(f\"    • False Negatives: {cm[1,0]}\")\n",
    "    print(f\"    • True Positives: {cm[1,1]}\")\n",
    "    \n",
    "    # Plot Confusion Matrix Heatmap\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name.replace(\"_\", \" \").title()}')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot ROC-AUC Curve\n",
    "    if y_pred_proba is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC-AUC Curve - {model_name.replace(\"_\", \" \").title()}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot Precision-Recall Curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(recall, precision, color='darkorange', lw=2, label=f'Precision-Recall Curve (AUC = {pr_auc:.2f})')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title(f'Precision-Recall Curve - {model_name.replace(\"_\", \" \").title()}')\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.show()\n",
    "    \n",
    "    return metrics, y_pred, y_pred_proba\n",
    "\n",
    "def train_and_evaluate_models(X, y, feature_names):\n",
    "    \"\"\"Train and evaluate multiple models with cross-validation\"\"\"\n",
    "    \n",
    "    print(\"\\n🚀 Starting Model Training and Evaluation...\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, \n",
    "        stratify=y if y.nunique() > 1 else None\n",
    "    )\n",
    "    \n",
    "    print(f\"  📊 Data Split:\")\n",
    "    print(f\"    • Training: {len(X_train)} samples\")\n",
    "    print(f\"    • Testing: {len(X_test)} samples\")\n",
    "    \n",
    "    # Create pipelines\n",
    "    pipelines, param_grids = create_ml_pipelines()\n",
    "    \n",
    "    # Store results\n",
    "    model_results = {}\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for model_name, pipeline in pipelines.items():\n",
    "        print(f\"\\n🔄 Training {model_name.replace('_', ' ').title()}...\")\n",
    "        \n",
    "        try:\n",
    "            # Hyperparameter tuning with GridSearchCV\n",
    "            cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline,\n",
    "                param_grids[model_name],\n",
    "                cv=cv_strategy,\n",
    "                scoring='f1',\n",
    "                n_jobs=1,  # Avoid multiprocessing issues in Databricks\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Fit the model\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_pipeline = grid_search.best_estimator_\n",
    "            \n",
    "            print(f\"  ✅ Best parameters: {grid_search.best_params_}\")\n",
    "            print(f\"  ✅ Best CV score: {grid_search.best_score_:.4f}\")\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            metrics, y_pred, y_pred_proba = evaluate_model_performance(\n",
    "                best_pipeline, X_test, y_test, model_name\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            model_results[model_name] = {\n",
    "                'model': best_pipeline,\n",
    "                'params': grid_search.best_params_,\n",
    "                'cv_score': grid_search.best_score_,\n",
    "                'metrics': metrics,\n",
    "                'predictions': y_pred,\n",
    "                'probabilities': y_pred_proba\n",
    "            }\n",
    "            \n",
    "            # Track best model\n",
    "            if metrics['f1'] > best_score:\n",
    "                best_score = metrics['f1']\n",
    "                best_model = model_name\n",
    "            \n",
    "            # Log to MLflow\n",
    "            try:\n",
    "                with mlflow.start_run(run_name=f\"{model_name}_improved\"):\n",
    "                    # Log parameters\n",
    "                    mlflow.log_params(grid_search.best_params_)\n",
    "                    \n",
    "                    # Log metrics\n",
    "                    mlflow.log_metrics(metrics)\n",
    "                    mlflow.log_metric(\"cv_score\", grid_search.best_score_)\n",
    "                    \n",
    "                    # Log model\n",
    "                    mlflow.sklearn.log_model(best_pipeline, f\"{model_name}_pipeline\")\n",
    "                    \n",
    "                    print(f\"  ✅ Logged to MLflow\")\n",
    "                    \n",
    "            except Exception as mlflow_error:\n",
    "                print(f\"  ⚠️  MLflow logging failed: {str(mlflow_error)}\")\n",
    "        \n",
    "        except Exception as model_error:\n",
    "            print(f\"  ❌ Error training {model_name}: {str(model_error)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n🏆 Model Training Summary:\")\n",
    "    print(f\"  • Trained {len(model_results)} models successfully\")\n",
    "    print(f\"  • Best model: {best_model} (F1 Score: {best_score:.4f})\")\n",
    "    \n",
    "    return model_results, best_model, X_test, y_test\n",
    "\n",
    "def create_model_comparison_visualization(model_results):\n",
    "    \"\"\"Create visualization comparing model performance\"\"\"\n",
    "    \n",
    "    print(\"\\n📊 Creating model comparison visualization...\")\n",
    "    \n",
    "    try:\n",
    "        # Extract metrics for comparison\n",
    "        models = list(model_results.keys())\n",
    "        metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        \n",
    "        # Create comparison plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Plot 1: Metrics comparison\n",
    "        metric_data = {}\n",
    "        for metric in metrics_to_plot:\n",
    "            metric_data[metric] = [model_results[model]['metrics'].get(metric, 0) for model in models]\n",
    "        \n",
    "        x = np.arange(len(models))\n",
    "        width = 0.2\n",
    "        \n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            axes[0].bar(x + i*width, metric_data[metric], width, label=metric.title())\n",
    "        \n",
    "        axes[0].set_xlabel('Models')\n",
    "        axes[0].set_ylabel('Score')\n",
    "        axes[0].set_title('Model Performance Comparison')\n",
    "        axes[0].set_xticks(x + width * 1.5)\n",
    "        axes[0].set_xticklabels([m.replace('_', ' ').title() for m in models])\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: CV Score vs Test F1 Score\n",
    "        cv_scores = [model_results[model]['cv_score'] for model in models]\n",
    "        test_f1_scores = [model_results[model]['metrics']['f1'] for model in models]\n",
    "        \n",
    "        axes[1].scatter(cv_scores, test_f1_scores, s=100, alpha=0.7)\n",
    "        for i, model in enumerate(models):\n",
    "            axes[1].annotate(model.replace('_', ' ').title(), \n",
    "                           (cv_scores[i], test_f1_scores[i]),\n",
    "                           xytext=(5, 5), textcoords='offset points')\n",
    "        \n",
    "        axes[1].set_xlabel('Cross-Validation F1 Score')\n",
    "        axes[1].set_ylabel('Test F1 Score')\n",
    "        axes[1].set_title('CV Score vs Test Performance')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add diagonal line for reference\n",
    "        min_score = min(min(cv_scores), min(test_f1_scores))\n",
    "        max_score = max(max(cv_scores), max(test_f1_scores))\n",
    "        axes[1].plot([min_score, max_score], [min_score, max_score], 'r--', alpha=0.5, label='Perfect Agreement')\n",
    "        axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"  ✅ Model comparison visualization created\")\n",
    "        \n",
    "    except Exception as viz_error:\n",
    "        print(f\"  ❌ Visualization error: {str(viz_error)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. EXECUTE ML PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "# Load and prepare data\n",
    "df = load_and_prepare_ml_data()\n",
    "df = create_target_variable(df)\n",
    "X, y, feature_names = prepare_features_for_ml(df)\n",
    "\n",
    "# Train and evaluate models\n",
    "model_results, best_model_name, X_test, y_test = train_and_evaluate_models(X, y, feature_names)\n",
    "\n",
    "# Create visualization\n",
    "if model_results:\n",
    "    create_model_comparison_visualization(model_results)\n",
    "\n",
    "# =============================================================================\n",
    "# 5. FEATURE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_feature_importance(model_results, feature_names, best_model_name):\n",
    "    \"\"\"Analyze and visualize feature importance\"\"\"\n",
    "    \n",
    "    print(f\"\\n🎯 Analyzing Feature Importance for {best_model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        best_model = model_results[best_model_name]['model']\n",
    "        \n",
    "        # Get feature importance (works for tree-based models)\n",
    "        if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
    "            importances = best_model.named_steps['classifier'].feature_importances_\n",
    "            \n",
    "            # Create feature importance dataframe\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(f\"  📊 Top 10 Most Important Features:\")\n",
    "            for i, (_, row) in enumerate(feature_importance_df.head(10).iterrows(), 1):\n",
    "                print(f\"    {i:2d}. {row['feature']}: {row['importance']:.4f}\")\n",
    "            \n",
    "            # Visualization\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            top_features = feature_importance_df.head(15)\n",
    "            sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')\n",
    "            plt.title(f'Feature Importance - {best_model_name.replace(\"_\", \" \").title()}')\n",
    "            plt.xlabel('Importance Score')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"  ✅ Feature importance analysis completed\")\n",
    "            return feature_importance_df\n",
    "            \n",
    "        else:\n",
    "            print(\"  ⚠️  Selected model doesn't support feature importance\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as fi_error:\n",
    "        print(f\"  ❌ Feature importance analysis failed: {str(fi_error)}\")\n",
    "        return None\n",
    "\n",
    "# Analyze feature importance\n",
    "if model_results and best_model_name:\n",
    "    feature_importance_df = analyze_feature_importance(model_results, feature_names, best_model_name)\n",
    "\n",
    "# =============================================================================\n",
    "# 6. SAVE PREDICTIONS AND MODEL ARTIFACTS\n",
    "# =============================================================================\n",
    "\n",
    "def save_predictions_to_spark(df, model_results, best_model_name):\n",
    "    \"\"\"Save model predictions back to Spark tables\"\"\"\n",
    "    \n",
    "    print(f\"\\n💾 Saving predictions to Spark tables...\")\n",
    "    \n",
    "    try:\n",
    "        if best_model_name and best_model_name in model_results:\n",
    "            # Get best model\n",
    "            best_model = model_results[best_model_name]['model']\n",
    "            \n",
    "            # Make predictions on full dataset\n",
    "            X_full, _, _ = prepare_features_for_ml(df)\n",
    "            predictions = best_model.predict(X_full)\n",
    "            prediction_proba = best_model.predict_proba(X_full)[:, 1]\n",
    "            \n",
    "            # Create predictions dataframe\n",
    "            predictions_df = pd.DataFrame({\n",
    "                'park_id': df['park_id'],\n",
    "                'intervention_pred': predictions,\n",
    "                'intervention_probability': prediction_proba\n",
    "            })\n",
    "            \n",
    "            # Convert to Spark DataFrame and save\n",
    "            predictions_spark = spark.createDataFrame(predictions_df)\n",
    "            predictions_spark.write.mode(\"overwrite\") \\\n",
    "                .saveAsTable(f\"{CATALOG}.{GOLD_SCHEMA}.urban_green_space_model_ce\")\n",
    "            \n",
    "            print(f\"✅ Predictions saved to {CATALOG}.{GOLD_SCHEMA}.urban_green_space_model_ce\")\n",
    "            \n",
    "            # Display sample predictions\n",
    "            print(f\"\\n📋 Sample Predictions:\")\n",
    "            predictions_spark.show(10)\n",
    "            \n",
    "            return predictions_spark\n",
    "        else:\n",
    "            print(\"❌ No valid model available for predictions\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as save_error:\n",
    "        print(f\"❌ Error saving predictions: {str(save_error)}\")\n",
    "        return None\n",
    "\n",
    "# Save predictions\n",
    "predictions_spark = save_predictions_to_spark(df, model_results, best_model_name)\n",
    "\n",
    "# =============================================================================\n",
    "# 7. MODEL SUMMARY AND NEXT STEPS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n🎯 ML PIPELINE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if model_results:\n",
    "    print(f\"✅ Successfully trained {len(model_results)} models\")\n",
    "    print(f\"🏆 Best performing model: {best_model_name}\")\n",
    "    \n",
    "    if best_model_name:\n",
    "        best_metrics = model_results[best_model_name]['metrics']\n",
    "        print(f\"📊 Best model performance:\")\n",
    "        for metric, value in best_metrics.items():\n",
    "            print(f\"  • {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"\\n📋 Model artifacts saved:\")\n",
    "    print(f\"  • Predictions: ml_project.default.urban_green_space_model_ce\")\n",
    "    print(f\"  • MLflow experiments: {experiment_name}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  No models were successfully trained\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Module 3: Machine learning pipeline & model development",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
