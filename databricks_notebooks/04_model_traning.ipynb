{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "304b8d86-519d-4c0b-bcf5-4dd6b904ba67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODULE 3: MACHINE LEARNING PIPELINE & MODEL DEVELOPMENT\n",
    "# Urban Green Space Management System \n",
    "# =============================================================================\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, auc\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== URBAN GREEN SPACE MANAGEMENT SYSTEM ===\")\n",
    "print(\"Module 3: Machine Learning Pipeline & Model Development (Improved)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. INITIALIZATION AND CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize Spark with optimized settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UGSM_ML_Pipeline_Improved\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configuration\n",
    "CATALOG = \"ml_project\"\n",
    "SILVER_SCHEMA = \"silver\"\n",
    "GOLD_SCHEMA = \"gold\"\n",
    "\n",
    "# MLflow configuration\n",
    "mlflow.set_registry_uri(\"databricks\")\n",
    "experiment_name = \"/Users/dilshanchanuka.bc@gmail.com/Urban Green Space Management\"\n",
    "\n",
    "try:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(f\" MLflow experiment set: {experiment_name}\")\n",
    "except Exception as e:\n",
    "    print(f\" MLflow experiment setup: {str(e)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA LOADING AND FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "def load_raw_data():\n",
    "    \"\"\"Load the raw dataset and display basic information\"\"\"\n",
    "    \n",
    "    print(\"\\n📊 Loading raw dataset...\")\n",
    "    \n",
    "    try:\n",
    "        # Load your dataset (adjust table name as needed)\n",
    "        df_spark = spark.read.table(f\"{CATALOG}.{SILVER_SCHEMA}.integrated\")\n",
    "        print(f\"✅ Loaded data from {CATALOG}.{SILVER_SCHEMA}.integrated\")\n",
    "        \n",
    "        # Display basic info\n",
    "        print(f\"📋 Dataset Info:\")\n",
    "        print(f\"  • Total records: {df_spark.count()}\")\n",
    "        print(f\"  • Total columns: {len(df_spark.columns)}\")\n",
    "        print(f\"  • Columns: {df_spark.columns}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\n📖 Sample Data:\")\n",
    "        df_spark.show(5, truncate=False)\n",
    "        \n",
    "        return df_spark\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_aggregated_features(df_spark):\n",
    "    \"\"\"Create aggregated features for ML from the raw dataset\"\"\"\n",
    "    \n",
    "    print(\"\\n🔧 Creating aggregated features...\")\n",
    "    \n",
    "    try:\n",
    "        # Aggregate data by park to create features\n",
    "        # park_features = df_spark.groupBy(\"park_id\", \"name\", \"city\", \"area_sqm\", \"latitude\", \"longitude\") \\\n",
    "        #     .agg(\n",
    "        #         # Air quality features\n",
    "        #         F.max(F.col(\"aqi\")).alias(\"max_aqi\"),\n",
    "        #         F.min(F.col(\"aqi\")).alias(\"min_aqi\"),\n",
    "        #         F.stddev(F.col(\"aqi\")).alias(\"std_aqi\"),\n",
    "        #         # ... rest of the aggregations ...\n",
    "        #     )\n",
    "        park_features = df_spark.groupBy(\"park_id\", \"name\", \"city\", \"area_sqm\", \"latitude\", \"longitude\") \\\n",
    "            .agg(\n",
    "                # Air quality features\n",
    "                F.max(F.col(\"aqi\")).alias(\"max_aqi\"),\n",
    "                F.min(F.col(\"aqi\")).alias(\"min_aqi\"),\n",
    "                F.avg(F.col(\"aqi\")).alias(\"avg_aqi\"),\n",
    "                F.stddev(F.col(\"aqi\")).alias(\"std_aqi\"),\n",
    "                # Visitor features\n",
    "                F.max(F.col(\"visitor_count\")).alias(\"max_visitors\"),\n",
    "                F.min(F.col(\"visitor_count\")).alias(\"min_visitors\"),\n",
    "                F.avg(F.col(\"visitor_count\")).alias(\"avg_visitors\"),\n",
    "                F.stddev(F.col(\"visitor_count\")).alias(\"std_visitors\"),\n",
    "                # Sentiment features\n",
    "                F.max(F.col(\"sentiment_score\")).alias(\"max_sentiment\"),\n",
    "                F.min(F.col(\"sentiment_score\")).alias(\"min_sentiment\"),\n",
    "                F.avg(F.col(\"sentiment_score\")).alias(\"avg_sentiment\"),\n",
    "                F.stddev(F.col(\"sentiment_score\")).alias(\"std_sentiment\"),\n",
    "                # Event features\n",
    "                F.sum(F.when(F.col(\"event_day\") == True, 1).otherwise(0)).alias(\"event_days_count\"),\n",
    "                F.count(\"*\").alias(\"total_records\"),\n",
    "                F.sum(F.col(\"visitor_count\")).alias(\"total_visitors\")\n",
    "            )\n",
    "        \n",
    "        # Add derived features\n",
    "        park_features_enhanced = park_features \\\n",
    "            .withColumn(\"aqi_range\", F.col(\"max_aqi\") - F.col(\"min_aqi\")) \\\n",
    "            .withColumn(\"sentiment_range\", F.col(\"max_sentiment\") - F.col(\"min_sentiment\")) \\\n",
    "            .withColumn(\"event_frequency\", F.col(\"event_days_count\") / F.col(\"total_records\")) \\\n",
    "            .withColumn(\"park_density\", F.col(\"total_visitors\") / F.col(\"area_sqm\")) \\\n",
    "            .fillna(0)  # Fill any null values with 0\n",
    "        \n",
    "        print(f\"✅ Created aggregated features\")\n",
    "        return park_features_enhanced\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating features: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_target_variable(df_spark):\n",
    "    \"\"\"Create intervention required target variable based on business logic\"\"\"\n",
    "    \n",
    "    print(\"\\n🎯 Creating target variable...\")\n",
    "    \n",
    "    try:\n",
    "        # Business logic for intervention requirement:\n",
    "        # Parks need intervention if they meet any of these criteria:\n",
    "        # 1. High air pollution (max AQI > 100) \n",
    "        # 2. Poor air quality AND negative sentiment (max AQI > 75 AND min sentiment < 0)\n",
    "        # 3. Very negative sentiment (min sentiment < -0.5)\n",
    "        # 4. Low visitor engagement despite good location (low visitors but large area)\n",
    "        \n",
    "        df_with_target = df_spark.withColumn(\n",
    "            \"intervention_required\",\n",
    "            when(\n",
    "                (col(\"max_aqi\") > 100) |\n",
    "                ((col(\"max_aqi\") > 75) & (col(\"min_sentiment\") < 0)) |\n",
    "                (col(\"min_sentiment\") < -0.5) |\n",
    "                ((col(\"max_visitors\") < 50) & (col(\"area_sqm\") > 10000)),\n",
    "                1\n",
    "            ).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        # Check target distribution\n",
    "        target_dist = df_with_target.groupBy(\"intervention_required\").count().collect()\n",
    "        total_count = df_with_target.count()\n",
    "        \n",
    "        print(f\"📊 Target Variable Distribution:\")\n",
    "        for row in target_dist:\n",
    "            label = \"No Intervention\" if row.intervention_required == 0 else \"Intervention Required\"\n",
    "            percentage = (row['count'] / total_count) * 100\n",
    "            print(f\"  • {label}: {row['count']} parks ({percentage:.1f}%)\")\n",
    "        \n",
    "        return df_with_target\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating target variable: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def prepare_ml_dataset(df_spark):\n",
    "    \"\"\"Prepare the final dataset for machine learning\"\"\"\n",
    "    \n",
    "    print(\"\\n🔧 Preparing ML dataset...\")\n",
    "    \n",
    "    try:\n",
    "        # Convert to Pandas for sklearn\n",
    "        df_pandas = df_spark.toPandas()\n",
    "        \n",
    "        # Define features to exclude from ML\n",
    "        exclude_cols = {\n",
    "            'park_id', 'name', 'city', 'intervention_required'\n",
    "        }\n",
    "        \n",
    "        # Get feature columns (all numeric columns except excluded ones)\n",
    "        all_cols = set(df_pandas.columns)\n",
    "        potential_features = all_cols - exclude_cols\n",
    "        \n",
    "        # Select only numeric features\n",
    "        numeric_features = []\n",
    "        for col in potential_features:\n",
    "            if df_pandas[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "                numeric_features.append(col)\n",
    "        \n",
    "        print(f\"📋 Selected {len(numeric_features)} numeric features:\")\n",
    "        for i, feat in enumerate(sorted(numeric_features), 1):\n",
    "            print(f\"  {i:2d}. {feat}\")\n",
    "        \n",
    "        # Prepare feature matrix and target\n",
    "        X = df_pandas[numeric_features].copy()\n",
    "        y = df_pandas['intervention_required']\n",
    "        \n",
    "        # Handle missing values\n",
    "        missing_info = X.isnull().sum()\n",
    "        if missing_info.sum() > 0:\n",
    "            print(f\"\\n🔧 Handling missing values:\")\n",
    "            for col in X.columns:\n",
    "                if missing_info[col] > 0:\n",
    "                    median_val = X[col].median()\n",
    "                    X[col] = X[col].fillna(median_val)\n",
    "                    print(f\"  • Filled {missing_info[col]} missing values in {col} with median: {median_val:.2f}\")\n",
    "        \n",
    "        # Handle infinite values\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "        X = X.fillna(X.median())\n",
    "        \n",
    "        print(f\"\\n✅ Final dataset prepared:\")\n",
    "        print(f\"  • Feature matrix shape: {X.shape}\")\n",
    "        print(f\"  • Target vector shape: {y.shape}\")\n",
    "        print(f\"  • Target distribution: {y.value_counts().to_dict()}\")\n",
    "        \n",
    "        return X, y, numeric_features, df_pandas\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error preparing ML dataset: {str(e)}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# =============================================================================\n",
    "# 3. MODEL DEVELOPMENT AND TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "def create_ml_pipelines():\n",
    "    \"\"\"Create multiple ML pipelines for comparison\"\"\"\n",
    "    \n",
    "    print(\"\\n🤖 Creating ML pipelines...\")\n",
    "    \n",
    "    pipelines = {\n",
    "        'logistic_regression': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "        ]),\n",
    "        \n",
    "        'random_forest': Pipeline([\n",
    "            ('scaler', RobustScaler()),  \n",
    "            ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "        ]),\n",
    "        \n",
    "        'gradient_boosting': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    # Hyperparameter grids\n",
    "    param_grids = {\n",
    "        'logistic_regression': {\n",
    "            'classifier__C': [0.1, 1.0, 10.0],\n",
    "            'classifier__penalty': ['l1', 'l2'],\n",
    "            'classifier__solver': ['liblinear']\n",
    "        },\n",
    "        \n",
    "        'random_forest': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__max_depth': [5, 10, None],\n",
    "            'classifier__min_samples_split': [2, 5, 10]\n",
    "        },\n",
    "        \n",
    "        'gradient_boosting': {\n",
    "            'classifier__n_estimators': [50, 100],\n",
    "            'classifier__learning_rate': [0.1, 0.2],\n",
    "            'classifier__max_depth': [3, 5]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Created {len(pipelines)} ML pipelines\")\n",
    "    return pipelines, param_grids\n",
    "\n",
    "def evaluate_model_performance(model, X_test, y_test, model_name):\n",
    "    \"\"\"Comprehensive model evaluation with visualizations\"\"\"\n",
    "    \n",
    "    print(f\"\\n📊 Evaluating {model_name} Performance...\")\n",
    "    \n",
    "    try:\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, average='binary', zero_division=0),\n",
    "            'recall': recall_score(y_test, y_pred, average='binary', zero_division=0),\n",
    "            'f1': f1_score(y_test, y_pred, average='binary', zero_division=0)\n",
    "        }\n",
    "        \n",
    "        if y_pred_proba is not None and len(np.unique(y_test)) > 1:\n",
    "            try:\n",
    "                metrics['auc_roc'] = roc_auc_score(y_test, y_pred_proba)\n",
    "            except:\n",
    "                metrics['auc_roc'] = 0.5\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"📈 Performance Metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  • {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"📊 Confusion Matrix:\")\n",
    "        print(f\"  • True Negatives: {cm[0,0]}\")\n",
    "        print(f\"  • False Positives: {cm[0,1] if cm.shape[1] > 1 else 0}\")\n",
    "        print(f\"  • False Negatives: {cm[1,0] if cm.shape[0] > 1 else 0}\")\n",
    "        print(f\"  • True Positives: {cm[1,1] if cm.shape == (2,2) else 0}\")\n",
    "        \n",
    "        # Create visualizations\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        # Plot 1: Confusion Matrix\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "        axes[0].set_title(f'Confusion Matrix - {model_name.replace(\"_\", \" \").title()}')\n",
    "        axes[0].set_xlabel('Predicted Labels')\n",
    "        axes[0].set_ylabel('True Labels')\n",
    "        \n",
    "        # Plot 2: ROC Curve\n",
    "        if y_pred_proba is not None and len(np.unique(y_test)) > 1:\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "            axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "            axes[1].set_xlim([0.0, 1.0])\n",
    "            axes[1].set_ylim([0.0, 1.05])\n",
    "            axes[1].set_xlabel('False Positive Rate')\n",
    "            axes[1].set_ylabel('True Positive Rate')\n",
    "            axes[1].set_title(f'ROC Curve - {model_name.replace(\"_\", \" \").title()}')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 3: Precision-Recall Curve\n",
    "            precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            axes[2].plot(recall, precision, color='darkorange', lw=2, label=f'PR Curve (AUC = {pr_auc:.2f})')\n",
    "            axes[2].set_xlim([0.0, 1.0])\n",
    "            axes[2].set_ylim([0.0, 1.05])\n",
    "            axes[2].set_xlabel('Recall')\n",
    "            axes[2].set_ylabel('Precision')\n",
    "            axes[2].set_title(f'Precision-Recall Curve - {model_name.replace(\"_\", \" \").title()}')\n",
    "            axes[2].legend()\n",
    "            axes[2].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'ROC Curve not available\\n(insufficient class variation)', \n",
    "                        ha='center', va='center', transform=axes[1].transAxes)\n",
    "            axes[2].text(0.5, 0.5, 'PR Curve not available\\n(insufficient class variation)', \n",
    "                        ha='center', va='center', transform=axes[2].transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return metrics, y_pred, y_pred_proba\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in model evaluation: {str(e)}\")\n",
    "        return {}, None, None\n",
    "\n",
    "def train_and_evaluate_models(X, y, feature_names):\n",
    "    \"\"\"Train and evaluate multiple models with cross-validation\"\"\"\n",
    "    \n",
    "    print(\"\\n🚀 Starting Model Training and Evaluation...\")\n",
    "    \n",
    "    try:\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, \n",
    "            stratify=y if y.nunique() > 1 else None\n",
    "        )\n",
    "        \n",
    "        print(f\"📊 Data Split:\")\n",
    "        print(f\"  • Training: {len(X_train)} samples\")\n",
    "        print(f\"  • Testing: {len(X_test)} samples\")\n",
    "        print(f\"  • Training target distribution: {y_train.value_counts().to_dict()}\")\n",
    "        print(f\"  • Testing target distribution: {y_test.value_counts().to_dict()}\")\n",
    "        \n",
    "        # Create pipelines\n",
    "        pipelines, param_grids = create_ml_pipelines()\n",
    "        \n",
    "        # Store results\n",
    "        model_results = {}\n",
    "        best_model = None\n",
    "        best_score = 0\n",
    "        \n",
    "        # Train and evaluate each model\n",
    "        for model_name, pipeline in pipelines.items():\n",
    "            print(f\"\\n🔄 Training {model_name.replace('_', ' ').title()}...\")\n",
    "            \n",
    "            try:\n",
    "                # Hyperparameter tuning with GridSearchCV\n",
    "                cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "                \n",
    "                grid_search = GridSearchCV(\n",
    "                    pipeline,\n",
    "                    param_grids[model_name],\n",
    "                    cv=cv_strategy,\n",
    "                    scoring='f1',\n",
    "                    n_jobs=1,  \n",
    "                    verbose=0\n",
    "                )\n",
    "                \n",
    "                # Fit the model\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_pipeline = grid_search.best_estimator_\n",
    "                \n",
    "                print(f\"✅ Best parameters: {grid_search.best_params_}\")\n",
    "                print(f\"✅ Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "                # Infer the model signature\n",
    "                signature = infer_signature(X_train, best_pipeline.predict(X_train))\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                metrics, y_pred, y_pred_proba = evaluate_model_performance(\n",
    "                    best_pipeline, X_test, y_test, model_name\n",
    "                )\n",
    "                \n",
    "                # Store results\n",
    "                model_results[model_name] = {\n",
    "                    'model': best_pipeline,\n",
    "                    'params': grid_search.best_params_,\n",
    "                    'cv_score': grid_search.best_score_,\n",
    "                    'metrics': metrics,\n",
    "                    'predictions': y_pred,\n",
    "                    'probabilities': y_pred_proba\n",
    "                }\n",
    "                \n",
    "                # Track best model\n",
    "                if metrics.get('f1', 0) > best_score:\n",
    "                    best_score = metrics.get('f1', 0)\n",
    "                    best_model = model_name\n",
    "                \n",
    "                # Log to MLflow\n",
    "                try:\n",
    "                    with mlflow.start_run(run_name=f\"{model_name}_rewritten\"):\n",
    "                        # Log parameters\n",
    "                        mlflow.log_params(grid_search.best_params_)\n",
    "                        \n",
    "                        # Log metrics\n",
    "                        mlflow.log_metrics(metrics)\n",
    "                        mlflow.log_metric(\"cv_score\", grid_search.best_score_)\n",
    "                        \n",
    "                        # Log model\n",
    "                        # mlflow.sklearn.log_model(best_pipeline, f\"{model_name}_pipeline\")\n",
    "                        mlflow.sklearn.log_model(best_pipeline, f\"{model_name}_pipeline\", signature=signature)\n",
    "                        print(f\"✅ Logged to MLflow\")\n",
    "                        \n",
    "                except Exception as mlflow_error:\n",
    "                    print(f\"⚠️ MLflow logging failed: {str(mlflow_error)}\")\n",
    "            \n",
    "            except Exception as model_error:\n",
    "                print(f\"❌ Error training {model_name}: {str(model_error)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n🏆 Model Training Summary:\")\n",
    "        print(f\"  • Trained {len(model_results)} models successfully\")\n",
    "        if best_model:\n",
    "            print(f\"  • Best model: {best_model} (F1 Score: {best_score:.4f})\")\n",
    "        \n",
    "        return model_results, best_model, X_test, y_test\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in model training: {str(e)}\")\n",
    "        return {}, None, None, None\n",
    "\n",
    "\n",
    "def create_model_comparison_visualization(model_results):\n",
    "    \"\"\"Create visualization comparing model performance\"\"\"\n",
    "    \n",
    "    print(\"\\n📊 Creating model comparison visualization...\")\n",
    "    \n",
    "    try:\n",
    "        if not model_results:\n",
    "            print(\"⚠️ No model results to visualize\")\n",
    "            return\n",
    "        \n",
    "        # Extract metrics for comparison\n",
    "        models = list(model_results.keys())\n",
    "        metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        \n",
    "        # Create comparison plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, 7)) # Increased figure size for better readability\n",
    "        \n",
    "        # Plot 1: Metrics comparison (Bar Chart)\n",
    "        metric_data = {}\n",
    "        for metric in metrics_to_plot:\n",
    "            metric_data[metric] = [model_results[model]['metrics'].get(metric, 0) for model in models]\n",
    "        \n",
    "        x = np.arange(len(models))\n",
    "        width = 0.2\n",
    "        \n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            axes[0].bar(x + i*width - (len(metrics_to_plot)/2 - 0.5) * width, metric_data[metric], width, label=metric.title()) # Adjusted x for grouping\n",
    "        \n",
    "        axes[0].set_xlabel('Models')\n",
    "        axes[0].set_ylabel('Score')\n",
    "        axes[0].set_title('Model Performance Comparison')\n",
    "        axes[0].set_xticks(x) # Centered xticks\n",
    "        axes[0].set_xticklabels([m.replace('_', ' ').title() for m in models], rotation=45, ha='right') # Rotate for long names\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].set_ylim(0, 1) # Assuming scores are between 0 and 1\n",
    "        \n",
    "        # Plot 2: CV Score vs Test F1 Score (Scatter Plot with improved labels)\n",
    "        cv_scores = [model_results[model]['cv_score'] for model in models]\n",
    "        test_f1_scores = [model_results[model]['metrics'].get('f1', 0) for model in models]\n",
    "        \n",
    "        axes[1].scatter(cv_scores, test_f1_scores, s=150, alpha=0.8, edgecolors='w', linewidth=0.5, zorder=2) # Larger markers with white edge\n",
    "        \n",
    "        # Enhanced annotation for scatter plot\n",
    "        for i, model in enumerate(models):\n",
    "            label = model.replace('_', ' ').title()\n",
    "            x_coord = cv_scores[i]\n",
    "            y_coord = test_f1_scores[i]\n",
    "            \n",
    "            # --- Dynamic Offset Logic for Annotations ---\n",
    "            # This is a simple strategy. For more complex cases, you might need\n",
    "            # libraries like `adjustText` or more sophisticated collision detection.\n",
    "            \n",
    "            # Default offset\n",
    "            offset_x, offset_y = 0.005, 0.005 # Small positive offset (to the right and up)\n",
    "            ha_align = 'left' # Horizontal alignment\n",
    "            va_align = 'bottom' # Vertical alignment\n",
    "\n",
    "            # Apply specific offsets for potentially overlapping models\n",
    "            # You'll need to customize these based on your typical data values and specific model names\n",
    "            if 'random_forest' in model.lower():\n",
    "                offset_x, offset_y = -0.015, 0.005 # Example: move text left, slightly up\n",
    "                ha_align = 'right'\n",
    "            elif 'gradient_boosting' in model.lower():\n",
    "                offset_x, offset_y = 0.005, -0.015 # Example: move text right, slightly down\n",
    "                ha_align = 'left'\n",
    "            elif 'logistic_regression' in model.lower():\n",
    "                offset_x, offset_y = -0.01, -0.01 # Example: another adjustment\n",
    "                ha_align = 'right'\n",
    "\n",
    "            # Add more specific conditions if you have other models that clash\n",
    "            # Example: If two models have very close (x,y) points\n",
    "            # You might iterate through models and compare distances to apply offsets\n",
    "            # For simplicity, we are using hardcoded offsets for common problematic names\n",
    "\n",
    "            axes[1].annotate(label, \n",
    "                             (x_coord, y_coord),\n",
    "                             xytext=(x_coord + offset_x, y_coord + offset_y), # Apply dynamic offset\n",
    "                             textcoords='data', # Use 'data' for xytext to be in data coordinates\n",
    "                             arrowprops=dict(facecolor='black', shrink=0.05, width=0.5, headwidth=5, alpha=0.7),\n",
    "                             horizontalalignment=ha_align, \n",
    "                             verticalalignment=va_align,\n",
    "                             fontsize=9,\n",
    "                             bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"yellow\", ec=\"b\", lw=0.5, alpha=0.7)) # Added bbox for visibility\n",
    "        \n",
    "        axes[1].set_xlabel('Cross-Validation F1 Score')\n",
    "        axes[1].set_ylabel('Test F1 Score')\n",
    "        axes[1].set_title('CV Score vs Test Performance')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add diagonal line for reference\n",
    "        if cv_scores and test_f1_scores:\n",
    "            # Ensure the line covers the full range of data, slightly beyond min/max\n",
    "            min_score = min(min(cv_scores), min(test_f1_scores)) * 0.95\n",
    "            max_score = max(max(cv_scores), max(test_f1_scores)) * 1.05\n",
    "            \n",
    "            axes[1].plot([min_score, max_score], [min_score, max_score], 'r--', alpha=0.5, label='Perfect Agreement', zorder=1)\n",
    "            axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"✅ Model comparison visualization created\")\n",
    "        \n",
    "    except Exception as viz_error:\n",
    "        print(f\"❌ Visualization error: {str(viz_error)}\")\n",
    "\n",
    "def analyze_feature_importance(model_results, feature_names, best_model_name):\n",
    "    \"\"\"Analyze and visualize feature importance for tree-based models\"\"\"\n",
    "    \n",
    "    if not best_model_name or best_model_name not in model_results:\n",
    "        print(\"⚠️ No valid model available for feature importance analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n🎯 Analyzing Feature Importance for {best_model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        best_model = model_results[best_model_name]['model']\n",
    "        \n",
    "        # Get feature importance (works for tree-based models)\n",
    "        if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
    "            importances = best_model.named_steps['classifier'].feature_importances_\n",
    "            \n",
    "            # Create feature importance dataframe\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(f\"📊 Top 10 Most Important Features:\")\n",
    "            for i, (_, row) in enumerate(feature_importance_df.head(10).iterrows(), 1):\n",
    "                print(f\"  {i:2d}. {row['feature']}: {row['importance']:.4f}\")\n",
    "            \n",
    "            # Visualization\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            top_features = feature_importance_df.head(15)\n",
    "            sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')\n",
    "            plt.title(f'Feature Importance - {best_model_name.replace(\"_\", \" \").title()}')\n",
    "            plt.xlabel('Importance Score')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"✅ Feature importance analysis completed\")\n",
    "            return feature_importance_df\n",
    "            \n",
    "        else:\n",
    "            print(\"⚠️ Selected model doesn't support feature importance\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as fi_error:\n",
    "        print(f\"❌ Feature importance analysis failed: {str(fi_error)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_predictions_to_spark(df_pandas, model_results, best_model_name, X, spark):\n",
    "    \"\"\"Save model predictions back to Spark tables\"\"\"\n",
    "    \n",
    "    print(f\"\\n💾 Saving predictions to Spark tables...\")\n",
    "    \n",
    "    try:\n",
    "        if not best_model_name or best_model_name not in model_results:\n",
    "            print(\"❌ No valid model available for predictions\")\n",
    "            return None\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = model_results[best_model_name]['model']\n",
    "        \n",
    "        # Make predictions on full dataset\n",
    "        predictions = best_model.predict(X)\n",
    "        prediction_proba = best_model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        # Create predictions dataframe\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'park_id': df_pandas['park_id'],\n",
    "            'park_name': df_pandas['name'],\n",
    "            'city': df_pandas['city'],\n",
    "            'intervention_actual': df_pandas['intervention_required'],\n",
    "            'intervention_pred': predictions,\n",
    "            'intervention_probability': prediction_proba,\n",
    "            'model_name': best_model_name,\n",
    "            'prediction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        })\n",
    "        \n",
    "        # Convert to Spark DataFrame and save\n",
    "        predictions_spark = spark.createDataFrame(predictions_df)\n",
    "        predictions_spark.write.mode(\"overwrite\") \\\n",
    "            .saveAsTable(f\"{CATALOG}.{GOLD_SCHEMA}.urban_green_space_predictions\")\n",
    "        \n",
    "        print(f\"✅ Predictions saved to {CATALOG}.{GOLD_SCHEMA}.urban_green_space_predictions\")\n",
    "        \n",
    "        # Display sample predictions\n",
    "        print(f\"\\n📋 Sample Predictions:\")\n",
    "        predictions_spark.show(10, truncate=False)\n",
    "        \n",
    "        # Show prediction summary\n",
    "        pred_summary = predictions_spark.groupBy(\"intervention_pred\").count().collect()\n",
    "        print(f\"\\n📊 Prediction Summary:\")\n",
    "        for row in pred_summary:\n",
    "            label = \"No Intervention\" if row.intervention_pred == 0 else \"Intervention Recommended\"\n",
    "            print(f\"  • {label}: {row['count']} parks\")\n",
    "        \n",
    "        return predictions_spark\n",
    "        \n",
    "    except Exception as save_error:\n",
    "        print(f\"❌ Error saving predictions: {str(save_error)}\")\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# 4. EXECUTE COMPLETE ML PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def run_complete_ml_pipeline():\n",
    "    \"\"\"Execute the complete ML pipeline\"\"\"\n",
    "    \n",
    "    print(f\"\\n🚀 EXECUTING COMPLETE ML PIPELINE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load raw data\n",
    "        df_spark = load_raw_data()\n",
    "        if df_spark is None:\n",
    "            print(\"❌ Failed to load data. Pipeline terminated.\")\n",
    "            return None\n",
    "        \n",
    "        # Step 2: Create aggregated features\n",
    "        df_features = create_aggregated_features(df_spark)\n",
    "        if df_features is None:\n",
    "            print(\"❌ Failed to create features. Pipeline terminated.\")\n",
    "            return None\n",
    "        \n",
    "        # Step 3: Create target variable\n",
    "        df_with_target = create_target_variable(df_features)\n",
    "        if df_with_target is None:\n",
    "            print(\"❌ Failed to create target variable. Pipeline terminated.\")\n",
    "            return None\n",
    "        \n",
    "        # Step 4: Prepare ML dataset\n",
    "        X, y, feature_names, df_pandas = prepare_ml_dataset(df_with_target)\n",
    "        if X is None:\n",
    "            print(\"❌ Failed to prepare ML dataset. Pipeline terminated.\")\n",
    "            return None\n",
    "        \n",
    "        # Step 5: Train and evaluate models\n",
    "        model_results, best_model_name, X_test, y_test = train_and_evaluate_models(X, y, feature_names)\n",
    "        if not model_results:\n",
    "            print(\"❌ No models were successfully trained. Pipeline terminated.\")\n",
    "            return None\n",
    "        \n",
    "        # Step 6: Create model comparison visualization\n",
    "        create_model_comparison_visualization(model_results)\n",
    "        \n",
    "        # Step 7: Analyze feature importance\n",
    "        feature_importance_df = analyze_feature_importance(model_results, feature_names, best_model_name)\n",
    "        \n",
    "        # Step 8: Save predictions\n",
    "        predictions_spark = save_predictions_to_spark(df_pandas, model_results, best_model_name, X, spark)\n",
    "        \n",
    "        # Step 9: Final summary\n",
    "        print_final_summary(model_results, best_model_name, feature_importance_df)\n",
    "        \n",
    "        return {\n",
    "            'model_results': model_results,\n",
    "            'best_model': best_model_name,\n",
    "            'feature_importance': feature_importance_df,\n",
    "            'predictions': predictions_spark\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Pipeline execution failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def print_final_summary(model_results, best_model_name, feature_importance_df):\n",
    "    \"\"\"Print comprehensive final summary of the ML pipeline\"\"\"\n",
    "    \n",
    "    print(f\"\\n🎯 MACHINE LEARNING PIPELINE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if model_results:\n",
    "        print(f\"✅ Successfully trained {len(model_results)} models\")\n",
    "        \n",
    "        # Model performance summary\n",
    "        print(f\"\\n📊 MODEL PERFORMANCE SUMMARY:\")\n",
    "        print(f\"{'Model':<20} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for model_name, results in model_results.items():\n",
    "            metrics = results['metrics']\n",
    "            print(f\"{model_name.replace('_', ' ').title():<20} \"\n",
    "                  f\"{metrics.get('accuracy', 0):<10.4f} \"\n",
    "                  f\"{metrics.get('precision', 0):<10.4f} \"\n",
    "                  f\"{metrics.get('recall', 0):<10.4f} \"\n",
    "                  f\"{metrics.get('f1', 0):<10.4f}\")\n",
    "        \n",
    "        if best_model_name:\n",
    "            print(f\"\\n🏆 BEST PERFORMING MODEL: {best_model_name.replace('_', ' ').title()}\")\n",
    "            best_metrics = model_results[best_model_name]['metrics']\n",
    "            print(f\"   📈 Performance Metrics:\")\n",
    "            for metric, value in best_metrics.items():\n",
    "                print(f\"      • {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "        \n",
    "        # Feature importance summary\n",
    "        if feature_importance_df is not None:\n",
    "            print(f\"\\n🎯 TOP 5 MOST IMPORTANT FEATURES:\")\n",
    "            for i, (_, row) in enumerate(feature_importance_df.head(5).iterrows(), 1):\n",
    "                print(f\"   {i}. {row['feature']}: {row['importance']:.4f}\")\n",
    "        \n",
    "        print(f\"\\n📋 OUTPUTS GENERATED:\")\n",
    "        print(f\"   • Model artifacts saved to MLflow\")\n",
    "        print(f\"   • Predictions saved to: {CATALOG}.{GOLD_SCHEMA}.urban_green_space_predictions\")\n",
    "        print(f\"   • Performance visualizations generated\")\n",
    "        \n",
    "        print(f\"\\n💡 BUSINESS INSIGHTS:\")\n",
    "        print(f\"   • Model can predict which parks need intervention\")\n",
    "        print(f\"   • Air quality and visitor patterns are key factors\")\n",
    "        print(f\"   • Can be used for proactive park management\")\n",
    "        \n",
    "        print(f\"\\n🚀 NEXT STEPS:\")\n",
    "        print(f\"   • Deploy model to production environment\")\n",
    "        print(f\"   • Set up automated retraining pipeline\")\n",
    "        print(f\"   • Create monitoring dashboard for predictions\")\n",
    "        print(f\"   • Integrate with park management systems\")\n",
    "    \n",
    "    else:\n",
    "        print(\"⚠️ No models were successfully trained\")\n",
    "        print(\"   • Check data quality and feature engineering\")\n",
    "        print(\"   • Review target variable creation logic\")\n",
    "        print(\"   • Ensure sufficient data volume\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. DATA QUALITY CHECKS AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def perform_data_quality_checks(df_spark):\n",
    "    \"\"\"Perform comprehensive data quality checks\"\"\"\n",
    "    \n",
    "    print(f\"\\n🔍 PERFORMING DATA QUALITY CHECKS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Basic statistics\n",
    "        total_records = df_spark.count()\n",
    "        total_parks = df_spark.select(\"park_id\").distinct().count()\n",
    "        \n",
    "        print(f\"📊 Basic Statistics:\")\n",
    "        print(f\"   • Total records: {total_records:,}\")\n",
    "        print(f\"   • Unique parks: {total_parks:,}\")\n",
    "        print(f\"   • Average records per park: {total_records/total_parks:.1f}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        print(f\"\\n🔍 Missing Value Analysis:\")\n",
    "        for col in df_spark.columns:\n",
    "            null_count = df_spark.filter(df_spark[col].isNull()).count()\n",
    "            null_percentage = (null_count / total_records) * 100\n",
    "            if null_percentage > 0:\n",
    "                print(f\"   • {col}: {null_count:,} ({null_percentage:.1f}%)\")\n",
    "        \n",
    "        # Data range analysis for key columns\n",
    "        print(f\"\\n📈 Data Range Analysis:\")\n",
    "        \n",
    "        # AQI analysis\n",
    "        aqi_stats = df_spark.select(\"aqi\").describe().collect()\n",
    "        print(f\"   • AQI Range:\")\n",
    "        for stat in aqi_stats:\n",
    "            if stat.summary in ['min', 'max', 'mean']:\n",
    "                print(f\"     - {stat.summary.title()}: {float(stat.aqi):.2f}\")\n",
    "        \n",
    "        # Visitor count analysis\n",
    "        visitor_stats = df_spark.select(\"visitor_count\").describe().collect()\n",
    "        print(f\"   • Visitor Count Range:\")\n",
    "        for stat in visitor_stats:\n",
    "            if stat.summary in ['min', 'max', 'mean']:\n",
    "                print(f\"     - {stat.summary.title()}: {float(stat.visitor_count):.2f}\")\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        sentiment_stats = df_spark.select(\"sentiment_score\").describe().collect()\n",
    "        print(f\"   • Sentiment Score Range:\")\n",
    "        for stat in sentiment_stats:\n",
    "            if stat.summary in ['min', 'max', 'mean']:\n",
    "                print(f\"     - {stat.summary.title()}: {float(stat.sentiment_score):.3f}\")\n",
    "        \n",
    "        # Temporal coverage\n",
    "        date_range = df_spark.select(\"date\").agg(F.min(\"date\").alias(\"min_date\"), F.max(\"date\").alias(\"max_date\")).collect()[0]\n",
    "        print(f\"\\n📅 Temporal Coverage:\")\n",
    "        print(f\"   • Date range: {date_range['min_date']} to {date_range['max_date']}\")\n",
    "        \n",
    "        # Geographic coverage\n",
    "        city_count = df_spark.select(\"city\").distinct().count()\n",
    "        print(f\"\\n🌍 Geographic Coverage:\")\n",
    "        print(f\"   • Number of cities: {city_count}\")\n",
    "        \n",
    "        top_cities = df_spark.groupBy(\"city\").count().orderBy(\"count\", ascending=False).limit(5).collect()\n",
    "        print(f\"   • Top 5 cities by record count:\")\n",
    "        for i, row in enumerate(top_cities, 1):\n",
    "            print(f\"     {i}. {row.city}: {row['count']:,} records\")\n",
    "        \n",
    "        print(f\"\\n✅ Data quality checks completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in data quality checks: {str(e)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. EXECUTE COMPLETE PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "# Run the complete ML pipeline\n",
    "print(\"Starting Urban Green Space ML Pipeline...\")\n",
    "print(\"Dataset columns available:\", ['park_id', 'timestamp', 'date', 'day_of_week', 'visitor_count', \n",
    "                                   'event_day', 'hour', 'name', 'city', 'area_sqm', 'latitude', \n",
    "                                   'longitude', 'aqi', 'no2_level', 'pm25_level', 'o3_level', \n",
    "                                   'tweet_text', 'sentiment_label', 'sentiment_score'])\n",
    "\n",
    "# Execute the pipeline\n",
    "pipeline_results = run_complete_ml_pipeline()\n",
    "\n",
    "# Additional analysis if pipeline was successful\n",
    "if pipeline_results:\n",
    "    print(f\"\\n🎉 PIPELINE EXECUTION COMPLETED SUCCESSFULLY!\")\n",
    "    \n",
    "    # Perform data quality checks if raw data is available\n",
    "    try:\n",
    "        raw_data = spark.read.table(f\"{CATALOG}.{SILVER_SCHEMA}.integrated\")\n",
    "        perform_data_quality_checks(raw_data)\n",
    "    except:\n",
    "        print(\"⚠️ Could not perform data quality checks - raw data table not accessible\")\n",
    "else:\n",
    "    print(f\"\\n❌ PIPELINE EXECUTION FAILED\")\n",
    "    print(\"Please check the error messages above and ensure:\")\n",
    "    print(\"   • Data table exists and is accessible\")\n",
    "    print(\"   • Required columns are present in the dataset\")\n",
    "    print(\"   • Data has sufficient quality and volume\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_model_traning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
